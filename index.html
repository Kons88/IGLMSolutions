<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Generalized Linear Models Solutions</title>
    <link rel="stylesheet" href="styles.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <button class="toggle-sidebar" onclick="toggleSidebar()">☰ Menu</button>
        <div class="title-container">
            <h1 id="main-title">An Introduction to Generalized Linear Models - Solutions</h1>
            <h2 id="subtitle">by Konstantinos Kirillov</h2>
        </div>
    </header>

    <div class="container">
        <div class="sidebar">
            <h3>Chapter 1</h3>
            <a href="#" data-exercise="exercise-1-1">Exercise 1.1</a>
            <a href="#" data-exercise="exercise-1-2">Exercise 1.2</a>
            <a href="#" data-exercise="exercise-1-3">Exercise 1.3</a>
            <a href="#" data-exercise="exercise-1-4">Exercise 1.4</a>
            <a href="#" data-exercise="exercise-1-5">Exercise 1.5</a>
            <a href="#" data-exercise="exercise-1-6">Exercise 1.6</a>

            <h3>Chapter 2</h3>
            <a href="#" data-exercise="exercise-2-1">Exercise 2.1</a>
            <a href="#" data-exercise="exercise-2-2">Exercise 2.2</a>
            <a href="#" data-exercise="exercise-2-3">Exercise 2.3</a>
            <a href="#" data-exercise="exercise-2-4">Exercise 2.4</a>
            <a href="#" data-exercise="exercise-2-5">Exercise 2.5</a>

            <h3>Chapter 3</h3>
            <a href="#" data-exercise="exercise-3-1">Exercise 3.1</a>
            <a href="#" data-exercise="exercise-3-2">Exercise 3.2</a>
            <a href="#" data-exercise="exercise-3-3">Exercise 3.3</a>
            <a href="#" data-exercise="exercise-3-4">Exercise 3.4</a>
            <a href="#" data-exercise="exercise-3-5">Exercise 3.5</a>
            <a href="#" data-exercise="exercise-3-6">Exercise 3.6</a>
            <a href="#" data-exercise="exercise-3-7">Exercise 3.7</a>
            <a href="#" data-exercise="exercise-3-8">Exercise 3.8</a>
            <a href="#" data-exercise="exercise-3-9">Exercise 3.9</a>
            <a href="#" data-exercise="exercise-3-10">Exercise 3.10</a>
        </div>
        <div class="content-overlay" onclick="toggleSidebar()"></div>
        <div class="content">
            <section id="exercise-1-1" class="active">
                <div class="problem">
                    <h3><u>Exercise 1.1:</u></h3>
                    <p>Let \(Y_1\) and \(Y_2\) be independent random variables with \(Y_1 \sim \mathcal{N}(1, 3)\) and \(Y_2 \sim \mathcal{N}(2, 5)\). If \(W_1 = Y_1 + 2Y_2\) and \(W_2 = 4Y_1 - Y_2\), what is the joint distribution of \(W_1\) and \(W_2\)?</p>
                </div>
                <div class="solution">
                    <h4><u>Solution:</u></h4>
                    <blockquote>
                      <p><i><u>A reminder from the book (Dobson, Annette J., 1945. An introduction to generalized linear models. Second Edition. pp. 14-15):</u></i></p>
                      <p>1. If the random variable \(Y\) has the Normal distribution with mean \(\mu\) and variance \(\sigma^2\), its probability density function is:
                         $$ f(y;\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}exp[-\frac{1}{2}(\frac{y-\mu}{\sigma^2})^2] $$
                         <span class="tab">We denote this by \(Y \sim \mathcal{N}(\mu,\sigma^2)\).</span>
                      </p>
                      <p>2. The Normal distribution with \(\mu = 0\) and \(\sigma^2 = 1\) \((Y \sim \mathcal{N}(0, 1))\) is called the <strong>standard Normal distribution</strong>.</p>
                      <p>3. Let \(Y_1,…,Y_n\) denote Normally distributed random variables with \(Y_i  \sim \mathcal{N}(\mu_i,\sigma_i^2)\) for \(i = 1,...,n\) and let the covariance of \(Y_i\) and \(Y_j\) be denoted by:
                         $$ cov(Y_i, Y_j)=\rho_{ij} \sigma_i \sigma_j, $$
                         <span class="tab">where \(ρ_{ij}\) is the correlation coefficient for \(Y_i\) and \(Y_j\). Then the joint distribution of the \(Y_i\)’s is the <strong>multivariate Normal distribution</strong> with mean vector \(\mu=[\mu_1,…,\mu_n ]^T\) and variance-covariance matrix \(V\) with diagonal elements \(\sigma_i^2\) and non-diagonal elements \(ρ_{ij} \sigma_i \sigma_j\) for \(i≠j\). We write this as:</span>
                         $$ y \sim \mathcal{N}(\mu, V), where \ \ y = [Y_1, ..., Y_n]^T $$
                      </p>
                      <p>
                         4. Suppose the random variables \(Y_1,…,Y_n\) are independent and normally distributed with the distributions \(Y_i \sim \mathcal{N}(\mu_i,\sigma_i^2)\) for \(i = 1,...,n\). If:
                         $$W = a_1Y_1 + a_2Y_2 + ... + a_nY_n, $$
                         <span class="tab">where the \(a_i\)’s are constants. Then \(W\) is also normally distributed, so that:</span>
                         $$ W=\sum_{i=1}^{n} a_i Y_i \sim \mathcal{N}(\sum_{i=1}^{n} a_i \mu_i \sum_{i=1}^{n} a_i^2 \sigma_i^2 ) $$
                      </p>
                    </blockquote>
                    <br />
                    <hr />
                    <br />
                    <p>It is apparent (from property 4) that the joint distribution of two normally distributed variables is yet another normal distribution. In this exercise, in order to find the joint distribution of \(W_1\) and \(W_2\), we first need to determine the mean, the variance and the covariance of \(W_1\) and \(W_2\) and then use those to derive the joint distribution.</p>
                    <br>
                    <p>Given that:</p>
                    <p>\(Y_1 \sim \mathcal{N}(1, 3)\) and </p>
                    <p>\(Y_2 \sim \mathcal{N}(2, 5)\)</p>
                    <br>
                    <p>First, let us find the means of \(W_1\) and \(W_2\):</p>
                    <p>\(E(W_1 )=E(Y_1+2 \cdot Y_2 )=E(Y_1 )+2 \cdot E(Y_2 )=1+2\cdot2=5 \Rightarrow E(W_1 )=5\)</p>
                    <p>\(E(W_2 )=E(4 \cdot Y_1-Y_2)=4 \cdot E(Y_1 )-E(Y_2 )=4\cdot1-2=2 \Rightarrow E(W_2 )=2\)</p>
                    <br>
                    <p>Next, let us calculate the variances of \(W_1\) and \(W_2\):</p>
                    <p>\(Var(W_1)=Var(Y_1+2 \cdot Y_2)=Var(Y_1)+2^2 \cdot Var(Y_2)=3+4\cdot5=23 \Rightarrow Var(W_1)=23\)</p>
                    <p>\(Var(W_2)=Var(4 \cdot Y_1-Y_2)=4^2 \cdot Var(Y_1)+Var(Y_2)=16 \cdot 3+5=53 \Rightarrow Var(W_2)=53\)</p>
                    <br>
                    <p>And finally, let us also compute the covariance between \(W_1\) and \(W_2\):</p>
                    <p>\(Cov(W_1,W_2)=Cov(Y_1+2 \cdot Y_2,4 \cdot Y_1-Y_2)=Cov(Y_1, 4 \dot Y_1)+Cov(Y_1,-Y_2)+Cov(2 \cdot Y_2, 4 \cdot Y_1 )+Cov(2 \cdot Y_2,-Y_2 )=\)
                    <p>\(=4 \cdot Var(Y_1)-Cov(Y_1,Y_2)+8 \cdot Cov(Y_2,Y_1)-2 \cdot Var(Y_2)=4 \cdot 3-0+8 \cdot 0-2 \cdot 5=2 \Rightarrow Cov(W_1,W_2 )=2 \)</p>
                    <br />
                    <p>Therefore, the joint distribution will be:</p>
                    <p>$$ \begin{bmatrix} W_1 \\ W_2 \end{bmatrix} \sim \mathcal{N} \left( \begin{bmatrix} 5 \\ 2 \end{bmatrix}, \begin{bmatrix} 23 & 2 \\ 2 & 53 \end{bmatrix} \right) $$</p>
                    <br />
                    <p>The correlation coefficient between \(W_1\) and \(W_2\) in this case shall be:</p>
                    <p>$$ \rho=\frac{Cov(W_1,W_2)}{\sigma_(W_1 ) \cdot \sigma_(W_2)}=\frac{Cov(W_1,W_2)}{\sqrt{Var(W_1)} \cdot \sqrt{Var(W_2)}}=\frac{2}{\sqrt{23} \cdot \sqrt{53}} \approx \frac{2}{4.8 \cdot 7.3} \approx 0.057 \Rightarrow  ρ=0.057 $$</p>
                    <br />
                    <p>Therefore, another way to express the joint distribution, would be:</p>
                    <p>$$ f(W_1,W_2)= \frac{1}{2 \cdot \pi \cdot \sigma_{W_1} \cdot \sigma_{W_2} \cdot \sqrt{1-ρ^2}} exp[-\frac{Z_{W_1}^2-2 \cdot Z_{W_1} \cdot Z_{W_2}+Z_{W_2}^2}{2 \cdot \sqrt{1-\rho^2}}] \Rightarrow $$</p>
                    <p>$$ \Rightarrow f(W_1,W_2)=\frac{1}{2 \cdot \pi \cdot 4.8 \cdot 7.3 \cdot \sqrt{1-0.057^2}} exp[-\frac{Z_{W_1}^2-2 \cdot Z_{W_1} \cdot Z_{W_2}+Z_{W_2}^2}{2 \cdot \sqrt{1-0.057^2 }}] $$</p>
                    <br />
                    <p>where:</p>
                    <p>\( Z_{W_1}=\frac{W_1 - \mu_{W_1} }{ \sigma_{W_1}} \)</p>
                    <p>\( Z_{W_2}=\frac{W_2 - \mu_{W_2} }{ \sigma_{W_2}} \)</p>
                    <br />
                </div>
            </section>

            <section id="exercise-1-2">
                <div class="problem">
                    <h3><u>Exercise 1.2:</u></h3>
                    <p>Let \(Y_1\) and \(Y_2\) be independent random variables with \(Y_1 \sim \mathcal{N}(0,1)\) and \(Y_2 \sim \mathcal{N}(3,4)\). </p>
                    <p><strong>a.</strong> What is the distribution of \(Y_1^2\)?</p>
                    <p><strong>b.</strong> If \(y=\begin{bmatrix} Y_1 \\ \frac{Y_2-3}{2} \end{bmatrix}\), obtain an expression for \(y^Ty\). What is its distribution?</p>
                    <p><strong>c.</strong> If \(y= \begin{bmatrix} Y_1 \\ Y_2 \end{bmatrix}\) and its distribution is \(y \sim \mathcal{N}(μ,V)\), obtain an expression for \(y^T V^{-1} y\). What is its distribution?</p>
                </div>
                <div class="solution">
                    <h4><u>Solution:</u></h4>
                    <blockquote>
                      <p><i><u>A reminder from the book (Dobson, Annette J., 1945. An introduction to generalized linear models. Second Edition. pp. 15-16):</u></i></p>
                      <p>1. The <strong>central chi-squared distribution</strong> with \(n\) degrees of freedom is defined as the sum of squares of \(n\) independent random variables \(Z_1,…,Z_n\) each with the standard Normal distribution. It is denoted by:</p>
                      <p> $$ \mathcal{X}^2= \sum_{i=1}^n Z_i^2 \sim \mathcal{\chi}_n^2. $$</p>
                      <span class="tab">In matrix notation, if \(z=[Z_1,…,Z_n ]^T\) then \(z^Tz=\sum_{i=1}^n Z_i^2\)  so that \( \mathcal{X}^2=z^T z \sim \chi_n^2\).</span>
                      <br />
                      <br />
                      <p>2. If \(\mathcal{X}^2\) has the distribution \(\chi^2 (n)\), then its expected value is \(E(\mathcal{X}^2)=n\) and its variance is \(Var(\mathcal{X}^2)=2 \cdot n\).</p>
                      <p>3. If \(Y_1,…,Y_n\) are independent Normally distributed random variables each with the distribution \(Y_i \sim \mathcal{N}(\mu_i,\sigma_i^2)\) then:</p>
                      <p>$$ \mathcal{X}^2= \sum_{i=1}^n (\frac{Y_i-\mu_i}{\sigma_i})^2 \sim \chi_n^2 $$</p>
                      <span class="tab">because each of the variables \(Z_i= \frac{Y_i-\mu_i}{\sigma_i}\) has the standard Normal distribution \(\mathcal{N}(0,1)\).</span>
                      <br />
                      <br />
                      <p>4.	Let \(Z_1,…,Z_n\) be independent random variables each with the distribution \(\mathcal{N}(0,1)\) and let \(Y_i=Z_i+\mu_i\), where at least one of the \(\mu_i\)’s is non-zero. Then the distribution of:</p>
                      <p>$$ \sum Y_i^2=\sum (Z_i+\mu_i)^2=\sum Z_i^2+2 \cdot \sum Z_i \cdot \mu_i+ \sum \mu_i^2  $$</p>
                      <span class="tab">has as larger mean \(n + \lambda\) and larger variance \(2 \cdot n + 4 \cdot \lambda\) than \(\chi_n^2\) where \(\lambda = \sum \mu_i^2\). This is called the <strong>non-central chi-squared distribution</strong> with \(n\) degrees of freedom and <strong>non-centrality parameter</strong> \(\lambda\). It is denoted by \(\chi_n^2 (\lambda)\).</span>
                      <br />
                      <br />
                      <p>5. Suppose that the \(Y_i\)’s are not necessarily independent and the vector \(y=[Y_1,…,Y_n ]^T\) has the multivariate normal distribution \(y \sim \mathcal{N}(\mu,V)\) where the variance-covariance matrix \(V\) is non-singular and its inverse is \(V^{-1}\). Then:</p>
                      <p>$$ \mathcal{X}^2=(y-\mu)^Τ V^{-1} (y-\mu) \sim \chi_n^2 $$</p>
                      <p>6. More generally if \(y \sim \mathcal{N}(\mu,V)\) then the random variable \(y^Τ V^{-1} y\) has the non-central chi-squared distribution \(\chi_n^2 (\lambda)\) where \(\lambda=\mu^Τ V^{-1} \mu\).</p>
                      <br />
                      <p>7. If \(X_1^2,…,X_m^2\) are \(m\) independent random variables with the chi-squared distributions \(\mathcal{X}_i^2 \sim \chi_{n_i}^2 (\lambda_i)\), which may or may not be central, then their sum also has a chi-squared distribution with \(\sum n_i\)  degrees of freedom and non-centrality parameter \(\sum \lambda_i\) , i.e.,</p>
                      <p>$$ \sum_{i=1}^m \mathcal{X}_i^2 \sim \chi_{\sum_{i=1}^m n_i}^2 (\sum_{i=1}^m \lambda_i) $$</p>
                      <span class="tab">This is called the <strong>reproductive property</strong> of the chi-squared distribution.</span>
                      <br />
                      <br />
                      <p>8.	Let \(y \sim \mathcal{N}(\mu,V)\), where \(y\) has \(n\) elements but the \(Y_i\)’s are not independent so that \(V\) is singular with rank \(k < n\) and the inverse of \(V\) is not uniquely defined. Let \(V^-\) denote a generalized inverse of \(V\). Then the random variable \(y^T V^- y\) has the non-central chi-squared distribution with \(k\) degrees of freedom and non-centrality parameter \(\lambda=\mu^Τ V^- \mu\).</p>
                    </blockquote>
                    <br />
                    <hr />
                    <br />
                    <p><strong>a.</strong> As property 1 from above would suggest, the chi-squared distribution with \(n\) degrees of freedom, \(\chi_n^2\), is the distribution of the sum of the squares of \(n\) independent standard normal random variables. If \(Y_1\) is a random variable following a normal distribution with mean \(\mu=0\) and variance \(\sigma^2=1\) \((Y_1 \sim \mathcal{N}(0,1))\), then the distribution of \(Y_1^2\) is a special case of the chi-squared distribution with one degree of freedom, \(\chi_1^2\). Meaning that:</p>
                    <p>$$ Y_1^2 \sim \chi_1^2 $$</p>
                    <blockquote>
                      <p><u>Sidenote:</u></p>
                      <p>The chi-squared distribution with 1 degree of freedom is sometimes referred to as the exponential distribution with rate parameter \(\lambda=2\) \((mean=\frac{1}{\lambda}=\frac{1}{2}, variance = \frac{1}{\lambda^2} = \frac{1}{4})\).</p>
                      <p>So, the distribution of \(Y_1^2\) is \(\chi_1^2\) or equivalently, an exponential distribution with rate parameter \(\lambda=2\).</p>
                    </blockquote>
                    <br />
                    <hr />
                    <br />
                    <p><strong>b.</strong> The expression \(y^T y\) is the dot product of the vector \(y\) with itself. So:</p>
                    <p>$$ y^T y=\begin{bmatrix} Y_1 & \frac{Y_2-3}{2} \end{bmatrix} \begin{bmatrix} Y_1 \\ \frac{Y_2-3}{2}\end{bmatrix}=Y_1^2+(\frac{Y_2-3}{2})^2 \Rightarrow y^T y=Y_1^2+ \frac{Y_2^2-6 \cdot Y_2+9}{4} $$</p>
                    <p>We know that \(Y_1 \sim \mathcal{N}(0,1)\) and \(Y_2 \sim \mathcal{N}(3,4)\), and that they are independent. We also know (form a) that \(Y_1^2\) is a special case of the chi-squared distribution with one degree of freedom, \(\chi_1^2\), or in other words: \(Y_1^2 \sim \chi_1^2\).</p>
                    <p>Furthermore, we are given that: \(Y_2 \sim \mathcal{N}(3,4)\), thus:</p>
                    <p>$$ Y_2 \sim \mathcal{N}(3,4) \Rightarrow Y_2-3 \sim \mathcal{N}(0,4) \Rightarrow \frac{Y_2-3}{2} \sim \mathcal{N}(0,1) \Rightarrow (\frac{Y_2-3}{2})^2 \sim \chi_1^2 $$</p>
                    <p>Since both \(Y_1^2\) and (\(\frac{Y_2-3}{2})^2\) are independent and follow a chi-squared distribution with 1 degree of freedom, then it follows that their sum will also follow the chi-squared distribution, but with two degrees of freedom, that are coming from the two terms combined. Therefore (and also according to property 3):</p>
                    <p>$$ y^T y=Y_1^2+ \frac{Y_2^2-6 \cdot Y_2+9}{4} \sim \chi_2^2 $$</p>
                    <br />
                    <hr />
                    <br />
                    <p><strong>c.</strong> We know that \(Y_1 \sim \mathcal{N}(0,1)\) and \(Y_2 \sim \mathcal{N}(3,4)\). Given that: \(y= \begin{bmatrix} Y_1 \\ Y_2 \end{bmatrix}\) and its distribution is \(y \sim \mathcal{N}(μ,V)\), we have that:</p>
                    <p>The mean vector \(\mu\) of \(y\), is:</p>
                    <p>\(\mu= \begin{bmatrix}0 \\ 3\end{bmatrix} \)</p>
                    <p>While the Variance-Covariance matrix \(V\), is a diagonal matrix, because \(Y_1\) and \(Y_2\) are independent and it is:</p>
                    <p>\( V= \begin{bmatrix}1 & 0 \\ 0 & 4\end{bmatrix} \)</p>
                    <p>Let us also compute the inverse of Variance-Covariance matrix \(V\), \(V^{-1}\), as it will be used:</p>
                    <blockquote>
                      <p><u>How to calculate the inverse of a 2x2 matrix:</u></p>
                      <p>A 2x2 matrix has the following general form:</p>
                      <p>\( A=\begin{bmatrix} a & b \\ c & d \end{bmatrix} \)</p>
                      <p>Its inverse then shall be:</p>
                      <p>\( A^{-1}=\frac{1}{det(A)} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix} = \frac{1}{a \cdot d - b \cdot c} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix} \)</p>
                      <p>Here, \(det(A)\) is the determinant of the matrix \(A\), and it must be non-zero for the inverse to exist. It becomes zero in the following cases: </p>
                      <ol class="tab">
                          <li>When the rows (or columns) of the matrix are linearly dependent, meaning one row is a scalar multiple of the other. For example, if the second row is \(k\) times the first row, then the matrix is singular, and the determinant is zero.</li>
                          <li>When the matrix represents a transformation that collapses the space into a lower dimension. For example, if the matrix represents a 2D transformation, a zero determinant means that the transformation squashes the 2D space into a 1D line, resulting in the loss of area (since the transformation is non-invertible). So if the determinant represents the area of the parallelogram formed by the row or column vectors, when the determinant is zero, the parallelogram collapses to a line or point, implying no area.</li>
                      </ol>
                    </blockquote>
                    <p>\( V^{-1}=\frac{1}{1 \cdot 4-0 \cdot 0} \begin{bmatrix}4&0\\0&1\end{bmatrix}=\frac{1}{4} \begin{bmatrix}4&0\\0&1\end{bmatrix} \Rightarrow V^{-1}= \begin{bmatrix}1&0\\0 & \frac{1}{4}\end{bmatrix} \)</p>
                    <p>Now, an expression for \(y^T V^{-1} y\), will be:</p>
                    <p>$$ y^T V^{-1} y= \begin{bmatrix}Y_1&Y_2 \end{bmatrix} \begin{bmatrix}1&0\\0 & \frac{1}{4}\end{bmatrix} \begin{bmatrix}Y_1 \\ Y_2 \end{bmatrix} =\begin{bmatrix}Y_1 & \frac{Y_2}{4} \end{bmatrix}\begin{bmatrix}Y_1 \\ Y_2 \end{bmatrix} \Rightarrow y^T V^{-1} y=Y_1^2+\frac{Y_2^2}{4} $$</p>
                    <p>As it was already shown above (in a), \(Y_1^2 \sim \chi_1^2\). Now, it was also shown (in b) that \((\frac{Y_2-3}{2})^2 \sim \chi_1^2\), and thus \(\frac{Y_2^2}{4} \sim \chi_1^2\), plus a non-centrality parameter \(\lambda\), which from property 6, is the following:</p>
                    <p>$$ \lambda=\mu^Τ V^{-1} \mu = \begin{bmatrix}0 & 3\end{bmatrix} \begin{bmatrix} 1&0 \\ 0 & \frac{1}{4}\end{bmatrix} \begin{bmatrix}0 \\ 3\end{bmatrix} = \begin{bmatrix}0 & \frac{3}{4}\end{bmatrix} \begin{bmatrix}0 \\ 3\end{bmatrix} \Rightarrow \lambda = \frac{9}{4} $$</p>
                    <p>And therefore, since we are adding two chi-squared distributed variables, with one degree of freedom each, it follows that (again from property 6):</p>
                    <p>$$ y^T V^{-1} y=Y_1^2+\frac{Y_2^2}{4} \sim \chi_2^2 (\frac{9}{4}) $$</p>
                    <br />
                    <br />
                </div>
            </section>

            <section id="exercise-1-3">
                <div class="problem">
                    <h3><u>Exercise 1.3:</u></h3>
                    <p>Let \(Y_1\) and \(Y_2\) be independent random variables with \(Y_1 \sim \mathcal{N}(1, 3)\) and \(Y_2 \sim \mathcal{N}(2, 5)\). If \(W_1 = Y_1 + 2Y_2\) and \(W_2 = 4Y_1 - Y_2\), what is the joint distribution of \(W_1\) and \(W_2\)?</p>
                </div>
                <div class="solution">
                    <h4><u>Solution:</u></h4>
                    <p>This is the solution for Exercise 1.3...</p>
                </div>
            </section>

            <section id="exercise-1-4">
                <div class="problem">
                    <h3><u>Exercise 1.4:</u></h3>
                    <p>Let \(Y_1\) and \(Y_2\) be independent random variables with \(Y_1 \sim \mathcal{N}(1, 3)\) and \(Y_2 \sim \mathcal{N}(2, 5)\). If \(W_1 = Y_1 + 2Y_2\) and \(W_2 = 4Y_1 - Y_2\), what is the joint distribution of \(W_1\) and \(W_2\)?</p>
                </div>
                <div class="solution">
                    <h4><u>Solution:</u></h4>
                    <p>This is the solution for Exercise 1.4...</p>
                </div>
            </section>

            <section id="exercise-1-5">
                <div class="problem">
                    <h3><u>Exercise 1.5:</u></h3>
                    <p>Let \(Y_1\) and \(Y_2\) be independent random variables with \(Y_1 \sim \mathcal{N}(1, 3)\) and \(Y_2 \sim \mathcal{N}(2, 5)\). If \(W_1 = Y_1 + 2Y_2\) and \(W_2 = 4Y_1 - Y_2\), what is the joint distribution of \(W_1\) and \(W_2\)?</p>
                </div>
                <div class="solution">
                    <h4><u>Solution:</u></h4>
                    <p>This is the solution for Exercise 1.5...</p>
                </div>
            </section>

            <section id="exercise-1-6">
                <div class="problem">
                    <h3><u>Exercise 1.6:</u></h3>
                    <p>Let \(Y_1\) and \(Y_2\) be independent random variables with \(Y_1 \sim \mathcal{N}(1, 3)\) and \(Y_2 \sim \mathcal{N}(2, 5)\). If \(W_1 = Y_1 + 2Y_2\) and \(W_2 = 4Y_1 - Y_2\), what is the joint distribution of \(W_1\) and \(W_2\)?</p>
                </div>
                <div class="solution">
                    <h4><u>Solution:</u></h4>
                    <p>This is the solution for Exercise 1.6...</p>
                </div>
            </section>


        </div>

    </div>

    <script>
        function updateHeaderText() {
        const mainTitle = document.getElementById("main-title");
        const subtitle = document.getElementById("subtitle");

        if (window.innerWidth <= 768) {
            mainTitle.textContent = "IGLM - Solutions";
            subtitle.style.display = "none";
        } else {
            mainTitle.textContent = "An Introduction to Generalized Linear Models - Solutions";
            subtitle.style.display = "block";
        }
        }

        function toggleSidebar() {
        const sidebar = document.querySelector('.sidebar');
        const overlay = document.querySelector('.content-overlay');
        sidebar.classList.toggle('active');
        overlay.classList.toggle('active');

        updateHeaderText(); // Update header text after toggling sidebar
        }

        document.addEventListener('click', function(event) {
        const sidebar = document.querySelector('.sidebar');
        const contentOverlay = document.querySelector('.content-overlay');

        if (!sidebar.contains(event.target) && !event.target.classList.contains('toggle-sidebar')) {
            sidebar.classList.remove('active');
            contentOverlay.classList.remove('active');
            updateHeaderText(); // Update header text when closing sidebar
        }
        });

        window.addEventListener("scroll", function() {
        const header = document.querySelector("header");
        if (window.scrollY > 50) {
            header.classList.add("scrolled");
        } else {
            header.classList.remove("scrolled");
        }
        });

        document.addEventListener("DOMContentLoaded", function() {
        document.querySelectorAll('.sidebar a').forEach(link => {
            link.addEventListener('click', function(e) {
                e.preventDefault();
                const exerciseId = this.getAttribute('data-exercise');

                document.querySelectorAll('.content section').forEach(section => {
                    section.classList.remove('active');
                });

                const targetSection = document.getElementById(exerciseId);
                if (targetSection) {
                    targetSection.classList.add('active');
                }

                if (window.innerWidth <= 768) {
                    toggleSidebar();
                }
            });
        });

        // Call updateHeaderText initially on load
        updateHeaderText();
        });

        // Call updateHeaderText on resize as well
        window.addEventListener("resize", updateHeaderText);
    </script>
</body>
</html>
