<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Generalized Linear Models Solutions</title>
    <link rel="stylesheet" href="styles.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <button class="toggle-sidebar" onclick="toggleSidebar()">☰ Menu</button>
        <div class="title-container">
            <h1 id="main-title">An Introduction to Generalized Linear Models - Solutions</h1>
            <h2 id="subtitle">by Konstantinos Kirillov</h2>
        </div>
    </header>

    <div class="container">
        <div class="sidebar">
            <h3>Chapter 1</h3>
            <a href="#" data-exercise="exercise-1-1">Exercise 1.1</a>
            <a href="#" data-exercise="exercise-1-2">Exercise 1.2</a>
            <a href="#" data-exercise="exercise-1-3">Exercise 1.3</a>
            <a href="#" data-exercise="exercise-1-4">Exercise 1.4</a>
            <a href="#" data-exercise="exercise-1-5">Exercise 1.5</a>
            <a href="#" data-exercise="exercise-1-6">Exercise 1.6</a>

            <h3>Chapter 2</h3>
            <a href="#" data-exercise="exercise-2-1">Exercise 2.1</a>
            <a href="#" data-exercise="exercise-2-2">Exercise 2.2</a>
            <a href="#" data-exercise="exercise-2-3">Exercise 2.3</a>
            <a href="#" data-exercise="exercise-2-4">Exercise 2.4</a>
            <a href="#" data-exercise="exercise-2-5">Exercise 2.5</a>

            <h3>Chapter 3</h3>
            <a href="#" data-exercise="exercise-3-1">Exercise 3.1</a>
            <a href="#" data-exercise="exercise-3-2">Exercise 3.2</a>
            <a href="#" data-exercise="exercise-3-3">Exercise 3.3</a>
            <a href="#" data-exercise="exercise-3-4">Exercise 3.4</a>
            <a href="#" data-exercise="exercise-3-5">Exercise 3.5</a>
            <a href="#" data-exercise="exercise-3-6">Exercise 3.6</a>
            <a href="#" data-exercise="exercise-3-7">Exercise 3.7</a>
            <a href="#" data-exercise="exercise-3-8">Exercise 3.8</a>
            <a href="#" data-exercise="exercise-3-9">Exercise 3.9</a>
            <a href="#" data-exercise="exercise-3-10">Exercise 3.10</a>
        </div>
        <div class="content-overlay" onclick="toggleSidebar()"></div>
        <div class="content">
            <section id="exercise-1-1" class="active">
                <div class="problem">
                    <h3><u>Exercise 1.1:</u></h3>
                    <p>Let \(Y_1\) and \(Y_2\) be independent random variables with \(Y_1 \sim \mathcal{N}(1, 3)\) and \(Y_2 \sim \mathcal{N}(2, 5)\). If \(W_1 = Y_1 + 2Y_2\) and \(W_2 = 4Y_1 - Y_2\), what is the joint distribution of \(W_1\) and \(W_2\)?</p>
                </div>
                <div class="solution">
                    <h4><u>Solution:</u></h4>
                    <blockquote>
                      <p><i><u>A reminder from the book (Dobson, Annette J., 1945. An introduction to generalized linear models. Second Edition. pp. 14-15):</u></i></p>
                      <p>1. If the random variable \(Y\) has the Normal distribution with mean \(\mu\) and variance \(\sigma^2\), its probability density function is:
                         $$ f(y;\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}exp[-\frac{1}{2}(\frac{y-\mu}{\sigma^2})^2] $$
                         <span class="tab">We denote this by \(Y \sim \mathcal{N}(\mu,\sigma^2)\).</span>
                      </p>
                      <p>2. The Normal distribution with \(\mu = 0\) and \(\sigma^2 = 1\) \((Y \sim \mathcal{N}(0, 1))\) is called the <strong>standard Normal distribution</strong>.</p>
                      <p>3. Let \(Y_1,…,Y_n\) denote Normally distributed random variables with \(Y_i  \sim \mathcal{N}(\mu_i,\sigma_i^2)\) for \(i = 1,...,n\) and let the covariance of \(Y_i\) and \(Y_j\) be denoted by:
                         $$ cov(Y_i, Y_j)=\rho_{ij} \sigma_i \sigma_j, $$
                         <span class="tab">where \(ρ_{ij}\) is the correlation coefficient for \(Y_i\) and \(Y_j\). Then the joint distribution of the \(Y_i\)’s is the <strong>multivariate Normal distribution</strong> with mean vector \(\mu=[\mu_1,…,\mu_n ]^T\) and variance-covariance matrix \(V\) with diagonal elements \(\sigma_i^2\) and non-diagonal elements \(ρ_{ij} \sigma_i \sigma_j\) for \(i≠j\). We write this as:</span>
                         $$ y \sim \mathcal{N}(\mu, V), where \ \ y = [Y_1, ..., Y_n]^T $$
                      </p>
                      <p>
                         4. Suppose the random variables \(Y_1,…,Y_n\) are independent and normally distributed with the distributions \(Y_i \sim \mathcal{N}(\mu_i,\sigma_i^2)\) for \(i = 1,...,n\). If:
                         $$W = a_1Y_1 + a_2Y_2 + ... + a_nY_n, $$
                         <span class="tab">where the \(a_i\)’s are constants. Then \(W\) is also normally distributed, so that:</span>
                         $$ W=\sum_{i=1}^{n} a_i Y_i \sim \mathcal{N}(\sum_{i=1}^{n} a_i \mu_i \sum_{i=1}^{n} a_i^2 \sigma_i^2 ) $$
                      </p>
                    </blockquote>
                    <br />
                    <hr />
                    <br />
                    <p>It is apparent (from property 4) that the joint distribution of two normally distributed variables is yet another normal distribution. In this exercise, in order to find the joint distribution of \(W_1\) and \(W_2\), we first need to determine the mean, the variance and the covariance of \(W_1\) and \(W_2\) and then use those to derive the joint distribution.</p>
                    <br>
                    <p>Given that:</p>
                    <p>\(Y_1 \sim \mathcal{N}(1, 3)\) and </p>
                    <p>\(Y_2 \sim \mathcal{N}(2, 5)\)</p>
                    <br>
                    <p>First, let us find the means of \(W_1\) and \(W_2\):</p>
                    <p>\(E(W_1 )=E(Y_1+2 \cdot Y_2 )=E(Y_1 )+2 \cdot E(Y_2 )=1+2\cdot2=5 \Rightarrow E(W_1 )=5\)</p>
                    <p>\(E(W_2 )=E(4 \cdot Y_1-Y_2)=4 \cdot E(Y_1 )-E(Y_2 )=4\cdot1-2=2 \Rightarrow E(W_2 )=2\)</p>
                    <br>
                    <p>Next, let us calculate the variances of \(W_1\) and \(W_2\):</p>
                    <p>\(Var(W_1)=Var(Y_1+2 \cdot Y_2)=Var(Y_1)+2^2 \cdot Var(Y_2)=3+4\cdot5=23 \Rightarrow Var(W_1)=23\)</p>
                    <p>\(Var(W_2)=Var(4 \cdot Y_1-Y_2)=4^2 \cdot Var(Y_1)+Var(Y_2)=16 \cdot 3+5=53 \Rightarrow Var(W_2)=53\)</p>
                    <br>
                    <p>And finally, let us also compute the covariance between \(W_1\) and \(W_2\):</p>
                    <p>\(Cov(W_1,W_2)=Cov(Y_1+2 \cdot Y_2,4 \cdot Y_1-Y_2)=Cov(Y_1, 4 \dot Y_1)+Cov(Y_1,-Y_2)+Cov(2 \cdot Y_2, 4 \cdot Y_1 )+Cov(2 \cdot Y_2,-Y_2 )=\)
                    <p>\(=4 \cdot Var(Y_1)-Cov(Y_1,Y_2)+8 \cdot Cov(Y_2,Y_1)-2 \cdot Var(Y_2)=4 \cdot 3-0+8 \cdot 0-2 \cdot 5=2 \Rightarrow Cov(W_1,W_2 )=2 \)</p>
                    <br />
                    <p>Therefore, the joint distribution will be:</p>
                    <p>$$ \begin{bmatrix} W_1 \\ W_2 \end{bmatrix} \sim \mathcal{N} \left( \begin{bmatrix} 5 \\ 2 \end{bmatrix}, \begin{bmatrix} 23 & 2 \\ 2 & 53 \end{bmatrix} \right) $$</p>
                    <br />
                    <p>The correlation coefficient between \(W_1\) and \(W_2\) in this case shall be:</p>
                    <p>$$ \rho=\frac{Cov(W_1,W_2)}{\sigma_(W_1 ) \cdot \sigma_(W_2)}=\frac{Cov(W_1,W_2)}{\sqrt{Var(W_1)} \cdot \sqrt{Var(W_2)}}=\frac{2}{\sqrt{23} \cdot \sqrt{53}} \approx \frac{2}{4.8 \cdot 7.3} \approx 0.057 \Rightarrow  ρ=0.057 $$</p>
                    <br />
                    <p>Therefore, another way to express the joint distribution, would be:</p>
                    <p>$$ f(W_1,W_2)= \frac{1}{2 \cdot \pi \cdot \sigma_{W_1} \cdot \sigma_{W_2} \cdot \sqrt{1-ρ^2}} exp[-\frac{Z_{W_1}^2-2 \cdot Z_{W_1} \cdot Z_{W_2}+Z_{W_2}^2}{2 \cdot \sqrt{1-\rho^2}}] \Rightarrow $$</p>
                    <p>$$ \Rightarrow f(W_1,W_2)=\frac{1}{2 \cdot \pi \cdot 4.8 \cdot 7.3 \cdot \sqrt{1-0.057^2}} exp[-\frac{Z_{W_1}^2-2 \cdot Z_{W_1} \cdot Z_{W_2}+Z_{W_2}^2}{2 \cdot \sqrt{1-0.057^2 }}] $$</p>
                    <br />
                    <p>where:</p>
                    <p>\( Z_{W_1}=\frac{W_1 - \mu_{W_1} }{ \sigma_{W_1}} \)</p>
                    <p>\( Z_{W_2}=\frac{W_2 - \mu_{W_2} }{ \sigma_{W_2}} \)</p>
                    <br />
                </div>
            </section>

            <section id="exercise-1-2">
                <div class="problem">
                    <h3><u>Exercise 1.2:</u></h3>
                    <p>Let \(Y_1\) and \(Y_2\) be independent random variables with \(Y_1 \sim \mathcal{N}(0,1)\) and \(Y_2 \sim \mathcal{N}(3,4)\). </p>
                    <p><strong>a.</strong> What is the distribution of \(Y_1^2\)?</p>
                    <p><strong>b.</strong> If \(y=\begin{bmatrix} Y_1 \\ \frac{Y_2-3}{2} \end{bmatrix}\), obtain an expression for \(y^Ty\). What is its distribution?</p>
                    <p><strong>c.</strong> If \(y= \begin{bmatrix} Y_1 \\ Y_2 \end{bmatrix}\) and its distribution is \(y \sim \mathcal{N}(μ,V)\), obtain an expression for \(y^T V^{-1} y\). What is its distribution?</p>
                </div>
                <div class="solution">
                    <h4><u>Solution:</u></h4>
                    <blockquote>
                      <p><i><u>A reminder from the book (Dobson, Annette J., 1945. An introduction to generalized linear models. Second Edition. pp. 15-16):</u></i></p>
                      <p>1. The <strong>central chi-squared distribution</strong> with \(n\) degrees of freedom is defined as the sum of squares of \(n\) independent random variables \(Z_1,…,Z_n\) each with the standard Normal distribution. It is denoted by:</p>
                      <p> $$ \mathcal{X}^2= \sum_{i=1}^n Z_i^2 \sim \mathcal{\chi}_n^2. $$</p>
                      <span class="tab">In matrix notation, if \(z=[Z_1,…,Z_n ]^T\) then \(z^Tz=\sum_{i=1}^n Z_i^2\)  so that \( \mathcal{X}^2=z^T z \sim \chi_n^2\).</span>
                      <br />
                      <br />
                      <p>2. If \(\mathcal{X}^2\) has the distribution \(\chi^2 (n)\), then its expected value is \(E(\mathcal{X}^2)=n\) and its variance is \(Var(\mathcal{X}^2)=2 \cdot n\).</p>
                      <p>3. If \(Y_1,…,Y_n\) are independent Normally distributed random variables each with the distribution \(Y_i \sim \mathcal{N}(\mu_i,\sigma_i^2)\) then:</p>
                      <p>$$ \mathcal{X}^2= \sum_{i=1}^n (\frac{Y_i-\mu_i}{\sigma_i})^2 \sim \chi_n^2 $$</p>
                      <span class="tab">because each of the variables \(Z_i= \frac{Y_i-\mu_i}{\sigma_i}\) has the standard Normal distribution \(\mathcal{N}(0,1)\).</span>
                      <br />
                      <br />
                      <p>4.	Let \(Z_1,…,Z_n\) be independent random variables each with the distribution \(\mathcal{N}(0,1)\) and let \(Y_i=Z_i+\mu_i\), where at least one of the \(\mu_i\)’s is non-zero. Then the distribution of:</p>
                      <p>$$ \sum Y_i^2=\sum (Z_i+\mu_i)^2=\sum Z_i^2+2 \cdot \sum Z_i \cdot \mu_i+ \sum \mu_i^2  $$</p>
                      <span class="tab">has as larger mean \(n + \lambda\) and larger variance \(2 \cdot n + 4 \cdot \lambda\) than \(\chi_n^2\) where \(\lambda = \sum \mu_i^2\). This is called the <strong>non-central chi-squared distribution</strong> with \(n\) degrees of freedom and <strong>non-centrality parameter</strong> \(\lambda\). It is denoted by \(\chi_n^2 (\lambda)\).</span>
                      <br />
                      <br />
                      <p>5. Suppose that the \(Y_i\)’s are not necessarily independent and the vector \(y=[Y_1,…,Y_n ]^T\) has the multivariate normal distribution \(y \sim \mathcal{N}(\mu,V)\) where the variance-covariance matrix \(V\) is non-singular and its inverse is \(V^{-1}\). Then:</p>
                      <p>$$ \mathcal{X}^2=(y-\mu)^Τ V^{-1} (y-\mu) \sim \chi_n^2 $$</p>
                      <p>6. More generally if \(y \sim \mathcal{N}(\mu,V)\) then the random variable \(y^Τ V^{-1} y\) has the non-central chi-squared distribution \(\chi_n^2 (\lambda)\) where \(\lambda=\mu^Τ V^{-1} \mu\).</p>
                      <br />
                      <p>7. If \(X_1^2,…,X_m^2\) are \(m\) independent random variables with the chi-squared distributions \(\mathcal{X}_i^2 \sim \chi_{n_i}^2 (\lambda_i)\), which may or may not be central, then their sum also has a chi-squared distribution with \(\sum n_i\)  degrees of freedom and non-centrality parameter \(\sum \lambda_i\) , i.e.,</p>
                      <p>$$ \sum_{i=1}^m \mathcal{X}_i^2 \sim \chi_{\sum_{i=1}^m n_i}^2 (\sum_{i=1}^m \lambda_i) $$</p>
                      <span class="tab">This is called the <strong>reproductive property</strong> of the chi-squared distribution.</span>
                      <br />
                      <br />
                      <p>8.	Let \(y \sim \mathcal{N}(\mu,V)\), where \(y\) has \(n\) elements but the \(Y_i\)’s are not independent so that \(V\) is singular with rank \(k < n\) and the inverse of \(V\) is not uniquely defined. Let \(V^-\) denote a generalized inverse of \(V\). Then the random variable \(y^T V^- y\) has the non-central chi-squared distribution with \(k\) degrees of freedom and non-centrality parameter \(\lambda=\mu^Τ V^- \mu\).</p>
                    </blockquote>
                    <br />
                    <hr />
                    <br />
                    <p><strong>a.</strong> As property 1 from above would suggest, the chi-squared distribution with \(n\) degrees of freedom, \(\chi_n^2\), is the distribution of the sum of the squares of \(n\) independent standard normal random variables. If \(Y_1\) is a random variable following a normal distribution with mean \(\mu=0\) and variance \(\sigma^2=1\) \((Y_1 \sim \mathcal{N}(0,1))\), then the distribution of \(Y_1^2\) is a special case of the chi-squared distribution with one degree of freedom, \(\chi_1^2\). Meaning that:</p>
                    <p>$$ Y_1^2 \sim \chi_1^2 $$</p>
                    <blockquote>
                      <p><u>Sidenote:</u></p>
                      <p>The chi-squared distribution with 1 degree of freedom is sometimes referred to as the exponential distribution with rate parameter \(\lambda=2\) \((mean=\frac{1}{\lambda}=\frac{1}{2}, variance = \frac{1}{\lambda^2} = \frac{1}{4})\).</p>
                      <p>So, the distribution of \(Y_1^2\) is \(\chi_1^2\) or equivalently, an exponential distribution with rate parameter \(\lambda=2\).</p>
                    </blockquote>
                    <br />
                    <hr />
                    <br />
                    <p><strong>b.</strong> The expression \(y^T y\) is the dot product of the vector \(y\) with itself. So:</p>
                    <p>$$ y^T y=\begin{bmatrix} Y_1 & \frac{Y_2-3}{2} \end{bmatrix} \begin{bmatrix} Y_1 \\ \frac{Y_2-3}{2}\end{bmatrix}=Y_1^2+(\frac{Y_2-3}{2})^2 \Rightarrow y^T y=Y_1^2+ \frac{Y_2^2-6 \cdot Y_2+9}{4} $$</p>
                    <p>We know that \(Y_1 \sim \mathcal{N}(0,1)\) and \(Y_2 \sim \mathcal{N}(3,4)\), and that they are independent. We also know (form a) that \(Y_1^2\) is a special case of the chi-squared distribution with one degree of freedom, \(\chi_1^2\), or in other words: \(Y_1^2 \sim \chi_1^2\).</p>
                    <p>Furthermore, we are given that: \(Y_2 \sim \mathcal{N}(3,4)\), thus:</p>
                    <p>$$ Y_2 \sim \mathcal{N}(3,4) \Rightarrow Y_2-3 \sim \mathcal{N}(0,4) \Rightarrow \frac{Y_2-3}{2} \sim \mathcal{N}(0,1) \Rightarrow (\frac{Y_2-3}{2})^2 \sim \chi_1^2 $$</p>
                    <p>Since both \(Y_1^2\) and (\(\frac{Y_2-3}{2})^2\) are independent and follow a chi-squared distribution with 1 degree of freedom, then it follows that their sum will also follow the chi-squared distribution, but with two degrees of freedom, that are coming from the two terms combined. Therefore (and also according to property 3):</p>
                    <p>$$ y^T y=Y_1^2+ \frac{Y_2^2-6 \cdot Y_2+9}{4} \sim \chi_2^2 $$</p>
                    <br />
                    <hr />
                    <br />
                    <p><strong>c.</strong> We know that \(Y_1 \sim \mathcal{N}(0,1)\) and \(Y_2 \sim \mathcal{N}(3,4)\). Given that: \(y= \begin{bmatrix} Y_1 \\ Y_2 \end{bmatrix}\) and its distribution is \(y \sim \mathcal{N}(μ,V)\), we have that:</p>
                    <p>The mean vector \(\mu\) of \(y\), is:</p>
                    <p>\(\mu= \begin{bmatrix}0 \\ 3\end{bmatrix} \)</p>
                    <p>While the Variance-Covariance matrix \(V\), is a diagonal matrix, because \(Y_1\) and \(Y_2\) are independent and it is:</p>
                    <p>\( V= \begin{bmatrix}1 & 0 \\ 0 & 4\end{bmatrix} \)</p>
                    <p>Let us also compute the inverse of Variance-Covariance matrix \(V\), \(V^{-1}\), as it will be used:</p>
                    <blockquote>
                      <p><u>How to calculate the inverse of a 2x2 matrix:</u></p>
                      <p>A 2x2 matrix has the following general form:</p>
                      <p>\( A=\begin{bmatrix} a & b \\ c & d \end{bmatrix} \)</p>
                      <p>Its inverse then shall be:</p>
                      <p>\( A^{-1}=\frac{1}{det(A)} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix} = \frac{1}{a \cdot d - b \cdot c} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix} \)</p>
                      <p>Here, \(det(A)\) is the determinant of the matrix \(A\), and it must be non-zero for the inverse to exist. It becomes zero in the following cases: </p>
                      <ol class="tab">
                          <li>When the rows (or columns) of the matrix are linearly dependent, meaning one row is a scalar multiple of the other. For example, if the second row is \(k\) times the first row, then the matrix is singular, and the determinant is zero.</li>
                          <li>When the matrix represents a transformation that collapses the space into a lower dimension. For example, if the matrix represents a 2D transformation, a zero determinant means that the transformation squashes the 2D space into a 1D line, resulting in the loss of area (since the transformation is non-invertible). So if the determinant represents the area of the parallelogram formed by the row or column vectors, when the determinant is zero, the parallelogram collapses to a line or point, implying no area.</li>
                      </ol>
                    </blockquote>
                    <p>\( V^{-1}=\frac{1}{1 \cdot 4-0 \cdot 0} \begin{bmatrix}4&0\\0&1\end{bmatrix}=\frac{1}{4} \begin{bmatrix}4&0\\0&1\end{bmatrix} \Rightarrow V^{-1}= \begin{bmatrix}1&0\\0 & \frac{1}{4}\end{bmatrix} \)</p>
                    <p>Now, an expression for \(y^T V^{-1} y\), will be:</p>
                    <p>$$ y^T V^{-1} y= \begin{bmatrix}Y_1&Y_2 \end{bmatrix} \begin{bmatrix}1&0\\0 & \frac{1}{4}\end{bmatrix} \begin{bmatrix}Y_1 \\ Y_2 \end{bmatrix} =\begin{bmatrix}Y_1 & \frac{Y_2}{4} \end{bmatrix}\begin{bmatrix}Y_1 \\ Y_2 \end{bmatrix} \Rightarrow y^T V^{-1} y=Y_1^2+\frac{Y_2^2}{4} $$</p>
                    <p>As it was already shown above (in a), \(Y_1^2 \sim \chi_1^2\). Now, it was also shown (in b) that \((\frac{Y_2-3}{2})^2 \sim \chi_1^2\), and thus \(\frac{Y_2^2}{4} \sim \chi_1^2\), plus a non-centrality parameter \(\lambda\), which from property 6, is the following:</p>
                    <p>$$ \lambda=\mu^Τ V^{-1} \mu = \begin{bmatrix}0 & 3\end{bmatrix} \begin{bmatrix} 1&0 \\ 0 & \frac{1}{4}\end{bmatrix} \begin{bmatrix}0 \\ 3\end{bmatrix} = \begin{bmatrix}0 & \frac{3}{4}\end{bmatrix} \begin{bmatrix}0 \\ 3\end{bmatrix} \Rightarrow \lambda = \frac{9}{4} $$</p>
                    <p>And therefore, since we are adding two chi-squared distributed variables, with one degree of freedom each, it follows that (again from property 6):</p>
                    <p>$$ y^T V^{-1} y=Y_1^2+\frac{Y_2^2}{4} \sim \chi_2^2 (\frac{9}{4}) $$</p>
                    <br />
                    <br />
                </div>
            </section>

            <section id="exercise-1-3">
                <div class="problem">
                    <h3><u>Exercise 1.3:</u></h3>
                    <p>Let the joint distribution of \(Y_1\) and \(Y_2\) be \(\mathcal{N}(\mu,V)\) with:</p>
                    <p>\( \mu=\begin{bmatrix} 2 \\ 3 \end{bmatrix}\)  and  \(V= \begin{bmatrix}4 & 1 \\ 1 & 9 \end{bmatrix} \)</p>
                    <p><strong>a.</strong> Obtain an expression for \((y-\mu)^T V^{-1} (y-\mu)\). What is its distribution?</p>
                    <p><strong>b.</strong> Obtain an expression for \(y^T V^{-1} y\). What is its distribution?</p>
                </div>
                <div class="solution">
                    <h4><u>Solution:</u></h4>
                    <blockquote>
                      <p><i><u>A reminder from the book (Dobson, Annette J., 1945. An introduction to generalized linear models. Second Edition. pp. 15-16):</u></i></p>
                      <p>1. The <strong>central chi-squared distribution</strong> with \(n\) degrees of freedom is defined as the sum of squares of \(n\) independent random variables \(Z_1,…,Z_n\) each with the standard Normal distribution. It is denoted by:</p>
                      <p> $$ \mathcal{X}^2= \sum_{i=1}^n Z_i^2 \sim \mathcal{\chi}_n^2. $$</p>
                      <span class="tab">In matrix notation, if \(z=[Z_1,…,Z_n ]^T\) then \(z^Tz=\sum_{i=1}^n Z_i^2\)  so that \( \mathcal{X}^2=z^T z \sim \chi_n^2\).</span>
                      <br />
                      <br />
                      <p>2. If \(\mathcal{X}^2\) has the distribution \(\chi^2 (n)\), then its expected value is \(E(\mathcal{X}^2)=n\) and its variance is \(Var(\mathcal{X}^2)=2 \cdot n\).</p>
                      <p>3. If \(Y_1,…,Y_n\) are independent Normally distributed random variables each with the distribution \(Y_i \sim \mathcal{N}(\mu_i,\sigma_i^2)\) then:</p>
                      <p>$$ \mathcal{X}^2= \sum_{i=1}^n (\frac{Y_i-\mu_i}{\sigma_i})^2 \sim \chi_n^2 $$</p>
                      <span class="tab">because each of the variables \(Z_i= \frac{Y_i-\mu_i}{\sigma_i}\) has the standard Normal distribution \(\mathcal{N}(0,1)\).</span>
                      <br />
                      <br />
                      <p>4.	Let \(Z_1,…,Z_n\) be independent random variables each with the distribution \(\mathcal{N}(0,1)\) and let \(Y_i=Z_i+\mu_i\), where at least one of the \(\mu_i\)’s is non-zero. Then the distribution of:</p>
                      <p>$$ \sum Y_i^2=\sum (Z_i+\mu_i)^2=\sum Z_i^2+2 \cdot \sum Z_i \cdot \mu_i+ \sum \mu_i^2  $$</p>
                      <span class="tab">has as larger mean \(n + \lambda\) and larger variance \(2 \cdot n + 4 \cdot \lambda\) than \(\chi_n^2\) where \(\lambda = \sum \mu_i^2\). This is called the <strong>non-central chi-squared distribution</strong> with \(n\) degrees of freedom and <strong>non-centrality parameter</strong> \(\lambda\). It is denoted by \(\chi_n^2 (\lambda)\).</span>
                      <br />
                      <br />
                      <p>5. Suppose that the \(Y_i\)’s are not necessarily independent and the vector \(y=[Y_1,…,Y_n ]^T\) has the multivariate normal distribution \(y \sim \mathcal{N}(\mu,V)\) where the variance-covariance matrix \(V\) is non-singular and its inverse is \(V^{-1}\). Then:</p>
                      <p>$$ \mathcal{X}^2=(y-\mu)^Τ V^{-1} (y-\mu) \sim \chi_n^2 $$</p>
                      <p>6. More generally if \(y \sim \mathcal{N}(\mu,V)\) then the random variable \(y^Τ V^{-1} y\) has the non-central chi-squared distribution \(\chi_n^2 (\lambda)\) where \(\lambda=\mu^Τ V^{-1} \mu\).</p>
                      <br />
                      <p>7. If \(X_1^2,…,X_m^2\) are \(m\) independent random variables with the chi-squared distributions \(\mathcal{X}_i^2 \sim \chi_{n_i}^2 (\lambda_i)\), which may or may not be central, then their sum also has a chi-squared distribution with \(\sum n_i\)  degrees of freedom and non-centrality parameter \(\sum \lambda_i\) , i.e.,</p>
                      <p>$$ \sum_{i=1}^m \mathcal{X}_i^2 \sim \chi_{\sum_{i=1}^m n_i}^2 (\sum_{i=1}^m \lambda_i) $$</p>
                      <span class="tab">This is called the <strong>reproductive property</strong> of the chi-squared distribution.</span>
                      <br />
                      <br />
                      <p>8.	Let \(y \sim \mathcal{N}(\mu,V)\), where \(y\) has \(n\) elements but the \(Y_i\)’s are not independent so that \(V\) is singular with rank \(k < n\) and the inverse of \(V\) is not uniquely defined. Let \(V^-\) denote a generalized inverse of \(V\). Then the random variable \(y^T V^- y\) has the non-central chi-squared distribution with \(k\) degrees of freedom and non-centrality parameter \(\lambda=\mu^Τ V^- \mu\).</p>
                    </blockquote>
                    <br />
                    <hr />
                    <br />
                    <p><strong>a.</strong> First, let us compute the inverse of Variance-Covariance matrix \(V\), \(V^{-1}\), as it will be needed. So:</p>
                    <blockquote>
                      <p><u>How to calculate the inverse of a 2x2 matrix:</u></p>
                      <p>A 2x2 matrix has the following general form:</p>
                      <p>\( A=\begin{bmatrix} a & b \\ c & d \end{bmatrix} \)</p>
                      <p>Its inverse then shall be:</p>
                      <p>\( A^{-1}=\frac{1}{det(A)} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix} = \frac{1}{a \cdot d - b \cdot c} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix} \)</p>
                      <p>Here, \(det(A)\) is the determinant of the matrix \(A\), and it must be non-zero for the inverse to exist. It becomes zero in the following cases: </p>
                      <ol class="tab">
                          <li>When the rows (or columns) of the matrix are linearly dependent, meaning one row is a scalar multiple of the other. For example, if the second row is \(k\) times the first row, then the matrix is singular, and the determinant is zero.</li>
                          <li>When the matrix represents a transformation that collapses the space into a lower dimension. For example, if the matrix represents a 2D transformation, a zero determinant means that the transformation squashes the 2D space into a 1D line, resulting in the loss of area (since the transformation is non-invertible). So if the determinant represents the area of the parallelogram formed by the row or column vectors, when the determinant is zero, the parallelogram collapses to a line or point, implying no area.</li>
                      </ol>
                    </blockquote>
                    <p>\( V^{-1}=\frac{1}{4 \cdot 9-1 \cdot 1} \begin{bmatrix} 9 & -1 \\ -1 & 4 \end{bmatrix} = \frac{1}{35} \begin{bmatrix} 9 & -1 \\ -1 & 4 \end{bmatrix} \Rightarrow V^{-1}=  \begin{bmatrix} \frac{9}{35} & -\frac{1}{35} \\ -\frac{1}{35} & \frac{4}{35} \end{bmatrix} \)</p>
                    <p>Since \(y= \begin{bmatrix} Y_1 \\ Y_2 \end{bmatrix}\) and \( \mu=\begin{bmatrix} 2 \\ 3 \end{bmatrix}\), then their difference shall be:</p>
                    <p>\( y-\mu=\begin{bmatrix} Y_1 \\ Y_2 \end{bmatrix}-\begin{bmatrix} 2 \\ 3 \end{bmatrix}=\begin{bmatrix} Y_1-2 \\ Y_2-3 \end{bmatrix} \)</p>
                    <p>And therefore, the joint distribution, will have the following form:</p>
                    <p>$$ (y-\mu)^T V^{-1} (y-\mu)= \begin{bmatrix} Y_1-2 & Y_2-3 \end{bmatrix} \begin{bmatrix} \frac{9}{35} & -\frac{1}{35} \\ -\frac{1}{35} & \frac{4}{35} \end{bmatrix} \begin{bmatrix} Y_1-2 \\ Y_2-3 \end{bmatrix}= $$</p>
                    <p>$$ = \begin{bmatrix} \frac{9}{35} (Y_1-2)-\frac{1}{35} (Y_2-3) & -\frac{1}{35} (Y_1-2)+ \frac{4}{35} (Y_2-3) \end{bmatrix} \begin{bmatrix} Y_1-2 \\ Y_2-3 \end{bmatrix}= $$</p>
                    <p>$$ = \frac{9}{35} (Y_1-2)^2-\frac{1}{35} (Y_2-3)(Y_1-2)-\frac{1}{35} (Y_1-2)(Y_2-3)+\frac{4}{35} (Y_2-3)^2 \Rightarrow $$</p>
                    <p>$$ \Rightarrow (y-μ)^T V^{-1} (y-μ)=\frac{9}{35} (Y_1-2)^2-\frac{2}{35} (Y_1-2)(Y_2-3)+\frac{4}{35} (Y_2-3)^2 $$</p>
                    <p>From property 5, we know that for a multivariate normal distribution \(y \sim \mathcal{N}(\mu,V)\), the quadratic form \((y-\mu)^T V^{-1} (y-\mu)\) follows a chi-squared distribution with degrees of freedom equal to the dimension of \(y\) (and in this case, we have only two dimensions), and therefore:</p>
                    <p>$$ (y-μ)^T V^{-1} (y-μ)=\frac{9}{35} (Y_1-2)^2-\frac{2}{35} (Y_1-2)(Y_2-3)+\frac{4}{35} (Y_2-3)^2 \sim \chi_2^2 $$</p>
                    <br />
                    <hr />
                    <br />
                    <p><strong>b.</strong> From the previous question (a), we already know that:</p>
                    <p>\( V^{-1}=  \begin{bmatrix} \frac{9}{35} & -\frac{1}{35} \\ -\frac{1}{35} & \frac{4}{35} \end{bmatrix} \)</p>
                    <p>And therefore, the expression for \(y^T V^{-1} y\) shall be:</p>
                    <p>$$ y^T V^{-1} y= \begin{bmatrix} Y_1 & Y_2 \end{bmatrix} \begin{bmatrix} \frac{9}{35} & -\frac{1}{35} \\ -\frac{1}{35} & \frac{4}{35} \end{bmatrix} \begin{bmatrix} Y_1 \\ Y_2 \end{bmatrix}= \begin{bmatrix} \frac{9}{35} \cdot Y_1-\frac{1}{35} \cdot Y_2 & -\frac{1}{35} \cdot Y_1 + \frac{4}{35} \cdot Y_2 \end{bmatrix} \begin{bmatrix} Y_1 \\ Y_2 \end{bmatrix}= $$</p>
                    <p>$$ = \frac{9}{35} \cdot Y_1^2-\frac{1}{35} \cdot Y_1 \cdot Y_2-\frac{1}{35} \cdot Y_2 \cdot Y_1  + \frac{4}{35} \cdot Y_2^2  \Rightarrow $$</p>
                    <p>$$ \Rightarrow y^T V^{-1} y = \frac{9}{35} \cdot Y_1^2-\frac{2}{35} \cdot Y_1 \cdot Y_2  +\frac{4}{35} \cdot Y_2^2 $$</p>
                    <p>Now, the distribution of \(y^T V^{-1} y\) is a more general case of the one described in the previous question (a) and thus follows property 6, meaning that: “if \(y \sim \mathcal{N}(μ,V)\) then the random variable \(y^Τ V^{-1} y\) has the non-central chi-squared distribution \(\chi_n^2 (\lambda)\) where \(\lambda=\mu^Τ V^{-1} \mu\).”</p>
                    <p>In our case, \(y\) can be written as \(y=\mu+Ζ\), where: \(Ζ \sim \mathcal{N}(0,V)\). So if we expanded on this, we would have:</p>
                    <p>\( y^T V^{-1} y=(\mu+Ζ)^T V^{-1} (\mu+Ζ)=\mu^T V^{-1} \mu+2 \cdot \mu^T V^{-1} Z+Z^T V^{-1} Z \)</p>
                    <p>with:</p>
                    <ul class="tab">
                        <li>	\(Z^T V^{-1} Z \sim \chi_2^2\), because \(Z \sim \mathcal{N}(0,V)\), and the quadratic form of a multivariate normal distribution follows a chi-squared distribution with degrees of freedom equal to the dimension of \(Z\) (which is 2).</li>
                        <li>	\(2 \cdot \mu^T V^{-1} Z\) is normally distributed with a mean of 0.</li>
                    </ul>
                    <br>
                    <br>
                    <p>Thus \(y^T V^{-1} y\) is a sum of a chi-squared distribution and a normal distribution. This means that \(y^T V^{-1} y\) follows a non-central chi-squared distribution with 2 degrees of freedom and a non-centrality parameter \(\lambda=\mu^T V^{-1} \mu\), which is:</p>
                    <p>$$ \lambda=\mu^T V^{-1} \mu=\begin{bmatrix} 2 & 3 \end{bmatrix} \begin{bmatrix} \frac{9}{35} & -\frac{1}{35} \\ -\frac{1}{35} & \frac{4}{35}) \end{bmatrix} \begin{bmatrix}2 \\ 3 \end{bmatrix}= \begin{bmatrix} \frac{18}{35}-\frac{3}{35} & -\frac{2}{35}+\frac{12}{35}\end{bmatrix} \begin{bmatrix} 2 \\ 3 \end{bmatrix}= $$</p>
                    <p>$$ = \begin{bmatrix} \frac{15}{35} & \frac{10}{35} \end{bmatrix} \begin{bmatrix} 2 \\ 3 \end{bmatrix} = \frac{30}{35}+\frac{30}{35}=\frac{60}{35} \Rightarrow \lambda=\frac{12}{7} $$</p>
                    <p>Therefore, in conclusion:</p>
                    <p>$$ y^T V^{-1} y = \frac{9}{35} \cdot Y_1^2-\frac{2}{35} \cdot Y_1 \cdot Y_2  +\frac{4}{35} \cdot Y_2^2 \sim \chi_2^2 (\frac{12}{7}) $$</p>
                    <br />
                    <br />
                </div>
            </section>

            <section id="exercise-1-4">
                <div class="problem">
                    <h3><u>Exercise 1.4:</u></h3>
                    <p>Let \(Y_1,…,Y_n\) be independent random variables each with the distribution \(\mathcal{N}(\mu,\sigma^2)\). Let:</p>
                    <p> $$ \overline{Y} = \frac{1}{n} \sum_{i=1}^n Y_i   \  \   and    \  \     S^2= \frac{1}{n-1} \sum_{i=1}^n (Y_i-\overline{Y})^2. $$</p>
                    <p><strong>a.</strong> What is the distribution of \(\overline{Y}\)?</p>
                    <p><strong>b.</strong> Show that \(S^2=\frac{1}{n-1} [\sum_{i=1}^n (Y_i-\mu)^2-n(\overline{Y}-\mu)^2 ]\).</p>
                    <p><strong>c.</strong> From (b) it follows that \(\sum\frac{(Y_i-\mu)^2}{σ^2} =(n-1)\frac{S^2}{\sigma^2} + [\frac{(\overline{Y}-\mu)^2 n}{\sigma^2} ]\). How does this allow you to deduce that \(Y\) and \(S^2\) are independent?</p>
                    <p><strong>d.</strong> What is the distribution of \((n-1)\frac{S^2}{\sigma^2}\)?</p>
                    <p><strong>e.</strong> What is the distribution of \(\frac{\overline{Y} - \mu}{\frac{S}{\sqrt{n}}}\) ?</p>
                </div>
                <div class="solution">
                    <h4><u>Solution:</u></h4>
                    <br />
                    <br />
                    <p><strong>a.</strong> Since the \(Y_i\) are independent and each has the distribution \(\mathcal{N}(\mu,\sigma^2)\), the expectation of \(\overline{Y}\) is:</p>
                    <p>\( Ε(\overline{Y})=Ε[\frac{1}{n} \sum_{i=1}^n Y_i ]=\frac{1}{n} \sum_{i=1}^n E[Y_i]= \frac{1}{n} \cdot n \cdot \mu=\mu \)</p>
                    <p>while its variance is:</p>
                    <p>\( Var(\overline{Y})=Var(\frac{1}{n} \sum_{i=1}^n Y_i)=\frac{1}{n}^2  \sum_{i=1}^n Var(Y_i) = \frac{1}{n^2} \cdot n \cdot \sigma^2=\frac{\sigma^2}{n} \)</p>
                    <p>We know that \(\overline{Y}\) consists of a linear combination of independent, normally distributed variables and therefore it is itself normally distributed. Thus, the distribution of \(\overline{Y}\) shall be:</p>
                    <p>$$ \overline{Y} \sim \mathcal{N}(\mu,\frac{\sigma^2}{n}) $$</p>
                    <br />
                    <hr />
                    <br />
                    <p><strong>b.</strong> Let us start from the definition of the sample variance:</p>
                    <p>$$ S^2 = \frac{1}{n-1} \sum_{i=1}^n (Y_i-\overline{Y})^2 = \frac{1}{n-1} \sum_{i=1}^n [(Y_i-\mu)-(\overline{Y}-\mu)]^2=$$</p>
                    <p>$$ = \frac{1}{n-1} \sum_{i=1}^n [(Y_i-\mu)^2 - 2 \cdot (Y_i-\mu)(\overline{Y}-\mu)+(\overline{Y}-\mu)^2 ] = $$</p>
                    <p>$$ = \frac{1}{n-1} [\sum_{i=1}^n(Y_i-\mu)^2 - \sum_{i=1}^n 2 \cdot (Y_i-\mu)(\overline{Y}-\mu)+\sum_{i=1}^n(\overline{Y}-\mu)^2]= $$</p>
                    <p>$$ = \frac{1}{n-1} [\sum_{i=1}^n(Y_i-\mu)^2 - 2 \cdot (\overline{Y}-\mu) \sum_{i=1}^n(Y_i-\mu)+\sum_{i=1}^n (\overline{Y}-\mu)^2]= $$</p>
                    <p>$$ = \frac{1}{n-1} [\sum_{i=1}^n(Y_i-\mu)^2 - 2 \cdot (\overline{Y}-\mu) \cdot n \cdot (\overline{Y}-\mu) + n \cdot (\overline{Y}-\mu)^2]= $$</p>
                    <p>$$ = \frac{1}{n-1} [\sum_{i=1}^n(Y_i-\mu)^2 - 2 \cdot n \cdot (\overline{Y}-\mu)^2 + n \cdot (\overline{Y}-\mu)^2 ] \Rightarrow $$</p>
                    <p>$$ \Rightarrow S^2 = \frac{1}{n-1} [\sum_{i=1}^n (Y_i-\mu)^2 - n \cdot (\overline{Y}-\mu)^2 ] $$</p>
                    <br />
                    <hr />
                    <br />
                    <p><strong>c.</strong> We are given the following expression:</p>
                    <p>$$ \frac{\sum_{i=1}^n (Y_i-\mu)^2}{\sigma^2} =\frac{(n-1) S^2}{\sigma^2} + \frac{n(\overline{Y}-\mu)^2}{\sigma^2}  $$</p>
                    <p>So, why are Y and S^2 independent? Let us look at the two right hand terms one by one.</p>
                    <br />
                    <p>Firstly, let us discuss the term:</p>
                    <p>\( \frac{(n-1) S^2}{\sigma^2} \)</p>
                    <p>Here \(S^2\) is the sample variance, which is defined as:</p>
                    <p>\( S^2= \frac{1}{n-1} \sum_{i=1}^n (Y_i-\overline{Y} )^2  \)</p>
                    <p>Therefore:</p>
                    <p>\( \frac{(n-1) S^2}{\sigma^2} = \frac{n-1}{\sigma^2} \cdot \frac{1}{n-1} \sum_{i=1}^n (Y_i-\overline{Y} )^2 = \frac{1}{\sigma^2}  \sum_{i=1}^n (Y_i-\overline{Y} )^2  \)</p>
                    <p>The sample variance measures the spread of the individual \(Y_i\)’s around the sample mean \(\overline{Y}\). This involves \(n-1\) degrees of freedom because the calculation of \(S^2\) depends on \(n\) data points, but the sample mean \(\overline{Y}\) is used to estimate the center of the data, reducing the degrees of freedom by 1.</p>
                    <p>Thus, under the assumption that the \(Y_i\)’s are normally distributed, the sum of squares, which was defined above, follows a chi-squared distribution with \(n-1\) degrees of freedom:</p>
                    <p>$$ \frac{(n-1) S^2}{\sigma^2} \sim \chi_{n-1}^2 $$</p>
                    <br />
                    <p>Secondly, let us discuss the term:</p>
                    <p>\( \frac{n(\overline{Y}-\mu)^2}{\sigma^2} \)</p>
                    <p>And from question <strong>a</strong>, we already know that:</p>
                    <p>\( \overline{Y} \sim \mathcal{N}(\mu,\frac{\sigma^2}{n}) \)</p>
                    <p>Therefore:</p>
                    <p>\( \frac{n(\overline{Y}-\mu)^2}{\sigma^2} = (\frac{\overline{Y}-\mu}{\frac{\sigma}{\sqrt{n}}})^2 \sim Ζ^2 \)</p>
                    <p>where \(Z\) is a standard normal random variable, \(Z \sim \mathcal{N}(0,1)\). And hence:</p>
                    <p>$$ \frac{n(\overline{Y}-\mu)^2}{\sigma^2} \sim \chi_1^2 $$</p>
                    <p>Since the total sum of squares can be split into two independent components, one involving \(\overline{Y}\) and the other involving \(S^2\), then by Cochran’s Theorem, the chi-squared terms must be independent. More formally, the independence of \(\chi_1^2\) and \(\chi_{n-1}^2\) implies that \(\overline{Y}\) and \(S^2\) are independent.</p>
                    <br>
                    <blockquote>
                      <p><u><i>Cochran’s Theorem:</i></u></p>
                      <p>Cochran's Theorem provides a way to decompose sums of squared normal random variables into independent chi-squared distributions. Specifically, if you have a set of independent normal random variables \(Y_1,Y_2,…,Y_n\) drawn from \(\mathcal{N}(\mu,\sigma^2)\). In our example, Cochran's Theorem states that the total sum of squares:</p>
                      <p>$$ \sum_{i=1}^n (Y_i-\mu)^2  $$</p>
                      <p>can be decomposed into two independent components:</p>
                      <p>\( \frac{n(\overline{Y}-\mu)^2}{\sigma^2} \sim \chi_1^2 \)</p>
                      <p>\( \frac{(n-1) S^2}{\sigma^2} \sim \chi_{n-1}^2\)</p>
                    </blockquote>
                    <br>
                    <p>This result is a key property of normal distributions and is a consequence of the fact that the sample mean \(\overline{Y}\) and sample variance \(S^2\) capture independent aspects of the data. \(\overline{Y}\) captures location (center), while \(S^2\) captures spread (variability) around the center.</p>
                    <br />
                    <hr />
                    <br />
                    <p><strong>d.</strong> As it was already shown in question <strong>c</strong>:</p>
                    <p>$$ \frac{(n-1) S^2}{\sigma^2} \sim \chi_{n-1}^2 $$</p>
                    <br />
                    <hr />
                    <br />
                    <p><strong>e.</strong> From question <strong>c</strong>, we got that:</p>
                    <p>\( \frac{n(\overline{Y}-\mu)^2}{\sigma^2} = (\frac{\overline{Y}-\mu}{\frac{\sigma}{\sqrt{n}}})^2 \sim Ζ^2 \Rightarrow \frac{\overline{Y}-\mu}{\frac{\sigma}{\sqrt{n}}} \sim \mathcal{N}(0,1) \)</p>
                    <p>\( \frac{(n-1) S^2}{\sigma^2} \sim \chi_{n-1}^2 \Rightarrow S \sim \frac{\sigma}{\sqrt{n-1}} \cdot \sqrt{ \chi_{n-1}^2 } \)</p>
                    <p>Thus, the numerator follows a standard normal distribution, while the denominator involves the sample standard deviation, which is related to the chi-squared distribution with \(n-1\) degrees of freedom. So, when we take the ratio of a standard normal random variable and the square root of a chi-squared random variable (divided by its degrees of freedom), the result follows a <strong>t-distribution</strong>. Hence:</p>
                    <p>$$ Τ = \frac{\overline{Y}-\mu}{S/\sqrt{n}} \sim \mathcal{t}_{n-1} $$</p>
                    <br />
                    <br />
                </div>
            </section>

            <section id="exercise-1-5">
                <div class="problem">
                    <h3><u>Exercise 1.5:</u></h3>
                    <p>Let \(Y_1\) and \(Y_2\) be independent random variables with \(Y_1 \sim \mathcal{N}(1, 3)\) and \(Y_2 \sim \mathcal{N}(2, 5)\). If \(W_1 = Y_1 + 2Y_2\) and \(W_2 = 4Y_1 - Y_2\), what is the joint distribution of \(W_1\) and \(W_2\)?</p>
                </div>
                <div class="solution">
                    <h4><u>Solution:</u></h4>
                    <p>This is the solution for Exercise 1.5...</p>
                </div>
            </section>

            <section id="exercise-1-6">
                <div class="problem">
                    <h3><u>Exercise 1.6:</u></h3>
                    <p>Let \(Y_1\) and \(Y_2\) be independent random variables with \(Y_1 \sim \mathcal{N}(1, 3)\) and \(Y_2 \sim \mathcal{N}(2, 5)\). If \(W_1 = Y_1 + 2Y_2\) and \(W_2 = 4Y_1 - Y_2\), what is the joint distribution of \(W_1\) and \(W_2\)?</p>
                </div>
                <div class="solution">
                    <h4><u>Solution:</u></h4>
                    <p>This is the solution for Exercise 1.6...</p>
                </div>
            </section>


        </div>

    </div>

    <script>
        function updateHeaderText() {
        const mainTitle = document.getElementById("main-title");
        const subtitle = document.getElementById("subtitle");

        if (window.innerWidth <= 768) {
            mainTitle.textContent = "IGLM - Solutions";
            subtitle.style.display = "none";
        } else {
            mainTitle.textContent = "An Introduction to Generalized Linear Models - Solutions";
            subtitle.style.display = "block";
        }
        }

        function toggleSidebar() {
        const sidebar = document.querySelector('.sidebar');
        const overlay = document.querySelector('.content-overlay');
        sidebar.classList.toggle('active');
        overlay.classList.toggle('active');

        updateHeaderText(); // Update header text after toggling sidebar
        }

        document.addEventListener('click', function(event) {
        const sidebar = document.querySelector('.sidebar');
        const contentOverlay = document.querySelector('.content-overlay');

        if (!sidebar.contains(event.target) && !event.target.classList.contains('toggle-sidebar')) {
            sidebar.classList.remove('active');
            contentOverlay.classList.remove('active');
            updateHeaderText(); // Update header text when closing sidebar
        }
        });

        window.addEventListener("scroll", function() {
        const header = document.querySelector("header");
        if (window.scrollY > 50) {
            header.classList.add("scrolled");
        } else {
            header.classList.remove("scrolled");
        }
        });

        document.addEventListener("DOMContentLoaded", function() {
        document.querySelectorAll('.sidebar a').forEach(link => {
            link.addEventListener('click', function(e) {
                e.preventDefault();
                const exerciseId = this.getAttribute('data-exercise');

                document.querySelectorAll('.content section').forEach(section => {
                    section.classList.remove('active');
                });

                const targetSection = document.getElementById(exerciseId);
                if (targetSection) {
                    targetSection.classList.add('active');
                }

                if (window.innerWidth <= 768) {
                    toggleSidebar();
                }
            });
        });

        // Call updateHeaderText initially on load
        updateHeaderText();
        });

        // Call updateHeaderText on resize as well
        window.addEventListener("resize", updateHeaderText);
    </script>
</body>
</html>
