<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>An Introduction to Generalized Linear Models - Solutions</title>
    <link rel="stylesheet" href="styles.css">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>An Introduction to Generalized Linear Models - Solutions</h1>
        <h2>by Konstantinos Kirillov</h2>
    </header>

    <div class="container">
        <nav id="sidebar">
            <ul>
                <li>
                    <h4>Chapter 1:</h4>
                    <ul>
                        <li><a href="#" onclick="showExercise('exercise1.1')">Exercise 1.1</a></li>
                        <li><a href="#" onclick="showExercise('exercise1.2')">Exercise 1.2</a></li>
                        <li><a href="#" onclick="showExercise('exercise1.3')">Exercise 1.3</a></li>
                        <li><a href="#" onclick="showExercise('exercise1.4')">Exercise 1.4</a></li>
                        <li><a href="#" onclick="showExercise('exercise1.5')">Exercise 1.5</a></li>
                        <li><a href="#" onclick="showExercise('exercise1.6')">Exercise 1.6</a></li>
                    </ul>
                </li>
                <h4>Chapter 2:</h4>
                <ul>
                    <li><a href="#" onclick="showExercise('exercise2.1')">Exercise 2.1</a></li>
                    <li><a href="#" onclick="showExercise('exercise2.2')">Exercise 2.2</a></li>
                    <li><a href="#" onclick="showExercise('exercise2.3')">Exercise 2.3</a></li>
                </ul>
            </ul>
        </nav>

        <main id="main-content">
            <section id="exercise1.1" class="exercise-section">
                <h3>Exercise 1.1:</h3>
                <p>Let \(Y_1\) and \(Y_2\) be independent random variables with \(Y_1 \sim \mathcal{N}(1, 3)\) and \(Y_2 \sim \mathcal{N}(2, 5)\). If \(W_1 = Y_1 + 2Y_2\) and \(W_2 = 4Y_1 - Y_2\), what is the joint distribution of \(W_1\) and \(W_2\)?</p>
                <h4>Solution:</h4>
                <br />
                <br />
                <blockquote>
                  <p><i><u>A reminder from the book (Dobson, Annette J., 1945. An introduction to generalized linear models. Second Edition. pp. 14-15):</u></i></p>
                  <p>1. If the random variable \(Y\) has the Normal distribution with mean \(\mu\) and variance \(\sigma^2\), its probability density function is:
                     $$ f(y;\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}exp[-\frac{1}{2}(\frac{y-\mu}{\sigma^2})^2] $$
                     <span class="tab">We denote this by \(Y \sim \mathcal{N}(\mu,\sigma^2)\).</span>
                  </p>
                  <p>2. The Normal distribution with \(\mu = 0\) and \(\sigma^2 = 1\) \((Y \sim \mathcal{N}(0, 1))\) is called the <strong>standard Normal distribution</strong>.</p>
                  <p>3. Let \(Y_1,…,Y_n\) denote Normally distributed random variables with \(Y_i  \sim \mathcal{N}(\mu_i,\sigma_i^2)\) for \(i = 1,...,n\) and let the covariance of \(Y_i\) and \(Y_j\) be denoted by:
                     $$ cov(Y_i, Y_j)=\rho_{ij} \sigma_i \sigma_j, $$
                     <span class="tab">where \(ρ_{ij}\) is the correlation coefficient for \(Y_i\) and \(Y_j\). Then the joint distribution of the \(Y_i\)’s is the <strong>multivariate Normal distribution</strong> with mean vector \(\mu=[\mu_1,…,\mu_n ]^T\) and variance-covariance matrix \(V\) with diagonal elements \(\sigma_i^2\) and non-diagonal elements \(ρ_{ij} \sigma_i \sigma_j\) for \(i≠j\). We write this as:</span>
                     $$ y \sim \mathcal{N}(\mu, V), where \ \ y = [Y_1, ..., Y_n]^T $$
                  </p>
                  <p>
                     4. Suppose the random variables \(Y_1,…,Y_n\) are independent and normally distributed with the distributions \(Y_i \sim \mathcal{N}(\mu_i,\sigma_i^2)\) for \(i = 1,...,n\). If:
                     $$W = a_1Y_1 + a_2Y_2 + ... + a_nY_n, $$
                     <span class="tab">where the \(a_i\)’s are constants. Then \(W\) is also normally distributed, so that:</span>
                     $$ W=\sum_{i=1}^{n} a_i Y_i \sim \mathcal{N}(\sum_{i=1}^{n} a_i \mu_i \sum_{i=1}^{n} a_i^2 \sigma_i^2 ) $$
                  </p>
                </blockquote>
                <br />
                <hr />
                <br />
                <p>It is apparent (from property 4) that the joint distribution of two normally distributed variables is yet another normal distribution. In this exercise, in order to find the joint distribution of \(W_1\) and \(W_2\), we first need to determine the mean, the variance and the covariance of \(W_1\) and \(W_2\) and then use those to derive the joint distribution.</p>
                <br>
                <p>Given that:</p>
                <p>\(Y_1 \sim \mathcal{N}(1, 3)\) and </p>
                <p>\(Y_2 \sim \mathcal{N}(2, 5)\)</p>
                <br>
                <p>First, let us find the means of \(W_1\) and \(W_2\):</p>
                <p>\(E(W_1 )=E(Y_1+2 \cdot Y_2 )=E(Y_1 )+2 \cdot E(Y_2 )=1+2\cdot2=5 \Rightarrow E(W_1 )=5\)</p>
                <p>\(E(W_2 )=E(4 \cdot Y_1-Y_2)=4 \cdot E(Y_1 )-E(Y_2 )=4\cdot1-2=2 \Rightarrow E(W_2 )=2\)</p>
                <br>
                <p>Next, let us calculate the variances of \(W_1\) and \(W_2\):</p>
                <p>\(Var(W_1)=Var(Y_1+2 \cdot Y_2)=Var(Y_1)+2^2 \cdot Var(Y_2)=3+4\cdot5=23 \Rightarrow Var(W_1)=23\)</p>
                <p>\(Var(W_2)=Var(4 \cdot Y_1-Y_2)=4^2 \cdot Var(Y_1)+Var(Y_2)=16 \cdot 3+5=53 \Rightarrow Var(W_2)=53\)</p>
                <br>
                <p>And finally, let us also compute the covariance between \(W_1\) and \(W_2\):</p>
                <p>\(Cov(W_1,W_2)=Cov(Y_1+2 \cdot Y_2,4 \cdot Y_1-Y_2)=Cov(Y_1, 4 \dot Y_1)+Cov(Y_1,-Y_2)+Cov(2 \cdot Y_2, 4 \cdot Y_1 )+Cov(2 \cdot Y_2,-Y_2 )=\)
                <p>\(=4 \cdot Var(Y_1)-Cov(Y_1,Y_2)+8 \cdot Cov(Y_2,Y_1)-2 \cdot Var(Y_2)=4 \cdot 3-0+8 \cdot 0-2 \cdot 5=2 \Rightarrow Cov(W_1,W_2 )=2 \)</p>
                <br />
                <p>Therefore, the joint distribution will be:</p>
                <p>$$ \begin{bmatrix} W_1 \\ W_2 \end{bmatrix} \sim \mathcal{N} \left( \begin{bmatrix} 5 \\ 2 \end{bmatrix}, \begin{bmatrix} 23 & 2 \\ 2 & 53 \end{bmatrix} \right) $$</p>
                <br />
                <p>The correlation coefficient between \(W_1\) and \(W_2\) in this case shall be:</p>
                <p>$$ \rho=\frac{Cov(W_1,W_2)}{\sigma_(W_1 ) \cdot \sigma_(W_2)}=\frac{Cov(W_1,W_2)}{\sqrt{Var(W_1)} \cdot \sqrt{Var(W_2)}}=\frac{2}{\sqrt{23} \cdot \sqrt{53}} \approx \frac{2}{4.8 \cdot 7.3} \approx 0.057 \Rightarrow  ρ=0.057 $$</p>
                <br />
                <p>Therefore, another way to express the joint distribution, would be:</p>
                <p>$$ f(W_1,W_2)= \frac{1}{2 \cdot \pi \cdot \sigma_{W_1} \cdot \sigma_{W_2} \cdot \sqrt{1-ρ^2}} exp[-\frac{Z_{W_1}^2-2 \cdot Z_{W_1} \cdot Z_{W_2}+Z_{W_2}^2}{2 \cdot \sqrt{1-\rho^2}}] \Rightarrow $$</p>
                <p>$$ \Rightarrow f(W_1,W_2)=\frac{1}{2 \cdot \pi \cdot 4.8 \cdot 7.3 \cdot \sqrt{1-0.057^2}} exp[-\frac{Z_{W_1}^2-2 \cdot Z_{W_1} \cdot Z_{W_2}+Z_{W_2}^2}{2 \cdot \sqrt{1-0.057^2 }}] $$</p>
                <br />
                <p>where:</p>
                <p>\( Z_{W_1}=\frac{W_1 - \mu_{W_1} }{ \sigma_{W_1}} \)</p>
                <p>\( Z_{W_2}=\frac{W_2 - \mu_{W_2} }{ \sigma_{W_2}} \)</p>
                <br />
            </section>

            <section id="exercise1.2" class="exercise-section">
                <h3>Exercise 1.2:</h3>
                <p>Let \(Y_1\) and \(Y_2\) be independent random variables with \(Y_1 \sim \mathcal{N}(0,1)\) and \(Y_2 \sim \mathcal{N}(3,4)\). </p>
                <p><strong>a.</strong> What is the distribution of \(Y_1^2\)?</p>
                <p><strong>b.</strong> If \(y=\begin{bmatrix} Y_1 \\ \frac{Y_2-3}{2} \end{bmatrix}\), obtain an expression for \(y^Ty\). What is its distribution?</p>
                <p><strong>c.</strong> If \(y= \begin{bmatrix} Y_1 \\ Y_2 \end{bmatrix}\) and its distribution is \(y \sim \mathcal{N}(μ,V)\), obtain an expression for \(y^T V^{-1} y\). What is its distribution?</p>
                <h4>Solution:</h4>
                <br />
                <br />
                <blockquote>
                  <p><i><u>A reminder from the book (Dobson, Annette J., 1945. An introduction to generalized linear models. Second Edition. pp. 15-16):</u></i></p>
                  <p>1. The <strong>central chi-squared distribution</strong> with \(n\) degrees of freedom is defined as the sum of squares of \(n\) independent random variables \(Z_1,…,Z_n\) each with the standard Normal distribution. It is denoted by:</p>
                  <p> $$ \mathcal{X}^2= \sum_{i=1}^n Z_i^2 \sim \mathcal{\chi}_n^2. $$</p>
                  <span class="tab">In matrix notation, if \(z=[Z_1,…,Z_n ]^T\) then \(z^Tz=\sum_{i=1}^n Z_i^2\)  so that \( \mathcal{X}^2=z^T z \sim \chi_n^2\).</span>
                  <br />
                  <br />
                  <p>2. If \(\mathcal{X}^2\) has the distribution \(\chi^2 (n)\), then its expected value is \(E(\mathcal{X}^2)=n\) and its variance is \(Var(\mathcal{X}^2)=2 \cdot n\).</p>
                  <p>3. If \(Y_1,…,Y_n\) are independent Normally distributed random variables each with the distribution \(Y_i \sim \mathcal{N}(\mu_i,\sigma_i^2)\) then:</p>
                  <p>$$ \mathcal{X}^2= \sum_{i=1}^n (\frac{Y_i-\mu_i}{\sigma_i})^2 \sim \chi_n^2 $$</p>
                  <span class="tab">because each of the variables \(Z_i= \frac{Y_i-\mu_i}{\sigma_i}\) has the standard Normal distribution \(\mathcal{N}(0,1)\).</span>
                  <br />
                  <br />
                  <p>4.	Let \(Z_1,…,Z_n\) be independent random variables each with the distribution \(\mathcal{N}(0,1)\) and let \(Y_i=Z_i+\mu_i\), where at least one of the \(\mu_i\)’s is non-zero. Then the distribution of:</p>
                  <p>$$ \sum Y_i^2=\sum (Z_i+\mu_i)^2=\sum Z_i^2+2 \cdot \sum Z_i \cdot \mu_i+ \sum \mu_i^2  $$</p>
                  <span class="tab">has as larger mean \(n + \lambda\) and larger variance \(2 \cdot n + 4 \cdot \lambda\) than \(\chi_n^2\) where \(\lambda = \sum \mu_i^2\). This is called the <strong>non-central chi-squared distribution</strong> with \(n\) degrees of freedom and <strong>non-centrality parameter</strong> \(\lambda\). It is denoted by \(\chi_n^2 (\lambda)\).</span>
                  <br />
                  <br />
                  <p>5. Suppose that the \(Y_i\)’s are not necessarily independent and the vector \(y=[Y_1,…,Y_n ]^T\) has the multivariate normal distribution \(y \sim \mathcal{N}(\mu,V)\) where the variance-covariance matrix \(V\) is non-singular and its inverse is \(V^{-1}\). Then:</p>
                  <p>$$ \mathcal{X}^2=(y-\mu)^Τ V^{-1} (y-\mu) \sim \chi_n^2 $$</p>
                  <p>6. More generally if \(y \sim \mathcal{N}(\mu,V)\) then the random variable \(y^Τ V^{-1} y\) has the non-central chi-squared distribution \(\chi_n^2 (\lambda)\) where \(\lambda=\mu^Τ V^{-1} \mu\).</p>
                  <br />
                  <p>7. If \(X_1^2,…,X_m^2\) are \(m\) independent random variables with the chi-squared distributions \(\mathcal{X}_i^2 \sim \chi_{n_i}^2 (\lambda_i)\), which may or may not be central, then their sum also has a chi-squared distribution with \(\sum n_i\)  degrees of freedom and non-centrality parameter \(\sum \lambda_i\) , i.e.,</p>
                  <p>$$ \sum_{i=1}^m \mathcal{X}_i^2 \sim \chi_{\sum_{i=1}^m n_i}^2 (\sum_{i=1}^m \lambda_i) $$</p>
                  <span class="tab">This is called the <strong>reproductive property</strong> of the chi-squared distribution.</span>
                  <br />
                  <br />
                  <p>8.	Let \(y \sim \mathcal{N}(\mu,V)\), where \(y\) has \(n\) elements but the \(Y_i\)’s are not independent so that \(V\) is singular with rank \(k < n\) and the inverse of \(V\) is not uniquely defined. Let \(V^-\) denote a generalized inverse of \(V\). Then the random variable \(y^T V^- y\) has the non-central chi-squared distribution with \(k\) degrees of freedom and non-centrality parameter \(\lambda=\mu^Τ V^- \mu\).</p>
                </blockquote>
                <br />
                <hr />
                <br />
                <p><strong>a.</strong> As property 1 from above would suggest, the chi-squared distribution with \(n\) degrees of freedom, \(\chi_n^2\), is the distribution of the sum of the squares of \(n\) independent standard normal random variables. If \(Y_1\) is a random variable following a normal distribution with mean \(\mu=0\) and variance \(\sigma^2=1\) \((Y_1 \sim \mathcal{N}(0,1))\), then the distribution of \(Y_1^2\) is a special case of the chi-squared distribution with one degree of freedom, \(\chi_1^2\). Meaning that:</p>
                <p>$$ Y_1^2 \sim \chi_1^2 $$</p>
                <blockquote>
                  <p><u>Sidenote:</u></p>
                  <p>The chi-squared distribution with 1 degree of freedom is sometimes referred to as the exponential distribution with rate parameter \(\lambda=2\) \((mean=\frac{1}{\lambda}=\frac{1}{2}, variance = \frac{1}{\lambda^2} = \frac{1}{4})\).</p>
                  <p>So, the distribution of \(Y_1^2\) is \(\chi_1^2\) or equivalently, an exponential distribution with rate parameter \(\lambda=2\).</p>
                </blockquote>
                <br />
                <hr />
                <br />
                <p><strong>b.</strong> The expression \(y^T y\) is the dot product of the vector \(y\) with itself. So:</p>
                <p>$$ y^T y=\begin{bmatrix} Y_1 & \frac{Y_2-3}{2} \end{bmatrix} \begin{bmatrix} Y_1 \\ \frac{Y_2-3}{2}\end{bmatrix}=Y_1^2+(\frac{Y_2-3}{2})^2 \Rightarrow y^T y=Y_1^2+ \frac{Y_2^2-6 \cdot Y_2+9}{4} $$</p>
                <p>We know that \(Y_1 \sim \mathcal{N}(0,1)\) and \(Y_2 \sim \mathcal{N}(3,4)\), and that they are independent. We also know (form a) that \(Y_1^2\) is a special case of the chi-squared distribution with one degree of freedom, \(\chi_1^2\), or in other words: \(Y_1^2 \sim \chi_1^2\).</p>
                <p>Furthermore, we are given that: \(Y_2 \sim \mathcal{N}(3,4)\), thus:</p>
                <p>$$ Y_2 \sim \mathcal{N}(3,4) \Rightarrow Y_2-3 \sim \mathcal{N}(0,4) \Rightarrow \frac{Y_2-3}{2} \sim \mathcal{N}(0,1) \Rightarrow (\frac{Y_2-3}{2})^2 \sim \chi_1^2 $$</p>
                <p>Since both \(Y_1^2\) and (\(\frac{Y_2-3}{2})^2\) are independent and follow a chi-squared distribution with 1 degree of freedom, then it follows that their sum will also follow the chi-squared distribution, but with two degrees of freedom, that are coming from the two terms combined. Therefore (and also according to property 3):</p>
                <p>$$ y^T y=Y_1^2+ \frac{Y_2^2-6 \cdot Y_2+9}{4} \sim \chi_2^2 $$</p>
                <br />
                <hr />
                <br />
                <p><strong>c.</strong> We know that \(Y_1 \sim \mathcal{N}(0,1)\) and \(Y_2 \sim \mathcal{N}(3,4)\). Given that: \(y= \begin{bmatrix} Y_1 \\ Y_2 \end{bmatrix}\) and its distribution is \(y \sim \mathcal{N}(μ,V)\), we have that:</p>
                <p>The mean vector \(\mu\) of \(y\), is:</p>
                <p>\(\mu= \begin{bmatrix}0 \\ 3\end{bmatrix} \)</p>
                <p>While the Variance-Covariance matrix \(V\), is a diagonal matrix, because \(Y_1\) and \(Y_2\) are independent and it is:</p>
                <p>\( V= \begin{bmatrix}1 & 0 \\ 0 & 4\end{bmatrix} \)</p>
                <p>Let us also compute the inverse of Variance-Covariance matrix \(V\), \(V^{-1}\), as it will be used:</p>
                <blockquote>
                  <p><u>How to calculate the inverse of a 2x2 matrix:</u></p>
                  <p>A 2x2 matrix has the following general form:</p>
                  <p>\( A=\begin{bmatrix} a & b \\ c & d \end{bmatrix} \)</p>
                  <p>Its inverse then shall be:</p>
                  <p>\( A^{-1}=\frac{1}{det(A)} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix} = \frac{1}{a \cdot d - b \cdot c} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix} \)</p>
                  <p>Here, \(det(A)\) is the determinant of the matrix \(A\), and it must be non-zero for the inverse to exist. It becomes zero in the following cases: </p>
                  <ol class="tab">
                      <li>When the rows (or columns) of the matrix are linearly dependent, meaning one row is a scalar multiple of the other. For example, if the second row is \(k\) times the first row, then the matrix is singular, and the determinant is zero.</li>
                      <li>When the matrix represents a transformation that collapses the space into a lower dimension. For example, if the matrix represents a 2D transformation, a zero determinant means that the transformation squashes the 2D space into a 1D line, resulting in the loss of area (since the transformation is non-invertible). So if the determinant represents the area of the parallelogram formed by the row or column vectors, when the determinant is zero, the parallelogram collapses to a line or point, implying no area.</li>
                  </ol>
                </blockquote>
                <p>\( V^{-1}=\frac{1}{1 \cdot 4-0 \cdot 0} \begin{bmatrix}4&0\\0&1\end{bmatrix}=\frac{1}{4} \begin{bmatrix}4&0\\0&1\end{bmatrix} \Rightarrow V^{-1}= \begin{bmatrix}1&0\\0 & \frac{1}{4}\end{bmatrix} \)</p>
                <p>Now, an expression for \(y^T V^{-1} y\), will be:</p>
                <p>$$ y^T V^{-1} y= \begin{bmatrix}Y_1&Y_2 \end{bmatrix} \begin{bmatrix}1&0\\0 & \frac{1}{4}\end{bmatrix} \begin{bmatrix}Y_1 \\ Y_2 \end{bmatrix} =\begin{bmatrix}Y_1 & \frac{Y_2}{4} \end{bmatrix}\begin{bmatrix}Y_1 \\ Y_2 \end{bmatrix} \Rightarrow y^T V^{-1} y=Y_1^2+\frac{Y_2^2}{4} $$</p>
                <p>As it was already shown above (in a), \(Y_1^2 \sim \chi_1^2\). Now, it was also shown (in b) that \((\frac{Y_2-3}{2})^2 \sim \chi_1^2\), and thus \(\frac{Y_2^2}{4} \sim \chi_1^2\), plus a non-centrality parameter \(\lambda\), which from property 6, is the following:</p>
                <p>$$ \lambda=\mu^Τ V^{-1} \mu = \begin{bmatrix}0 & 3\end{bmatrix} \begin{bmatrix} 1&0 \\ 0 & \frac{1}{4}\end{bmatrix} \begin{bmatrix}0 \\ 3\end{bmatrix} = \begin{bmatrix}0 & \frac{3}{4}\end{bmatrix} \begin{bmatrix}0 \\ 3\end{bmatrix} \Rightarrow \lambda = \frac{9}{4} $$</p>
                <p>And therefore, since we are adding two chi-squared distributed variables, with one degree of freedom each, it follows that (again from property 6):</p>
                <p>$$ y^T V^{-1} y=Y_1^2+\frac{Y_2^2}{4} \sim \chi_2^2 (\frac{9}{4}) $$</p>
                <br />
                <br />
            </section>

            <section id="exercise1.3" class="exercise-section">
                <h3>Exercise 1.3:</h3>
                <p>Let the joint distribution of \(Y_1\) and \(Y_2\) be \(\mathcal{N}(\mu,V)\) with:</p>
                <p>\( \mu=\begin{bmatrix} 2 \\ 3 \end{bmatrix}\)  and  \(V= \begin{bmatrix}4 & 1 \\ 1 & 9 \end{bmatrix} \)</p>
                <p><strong>a.</strong> Obtain an expression for \((y-\mu)^T V^{-1} (y-\mu)\). What is its distribution?</p>
                <p><strong>b.</strong> Obtain an expression for \(y^T V^{-1} y\). What is its distribution?</p>
                <h4>Solution:</h4>
                <br />
                <br />
                <blockquote>
                  <p><i><u>A reminder from the book (Dobson, Annette J., 1945. An introduction to generalized linear models. Second Edition. pp. 15-16):</u></i></p>
                  <p>1. The <strong>central chi-squared distribution</strong> with \(n\) degrees of freedom is defined as the sum of squares of \(n\) independent random variables \(Z_1,…,Z_n\) each with the standard Normal distribution. It is denoted by:</p>
                  <p> $$ \mathcal{X}^2= \sum_{i=1}^n Z_i^2 \sim \mathcal{\chi}_n^2. $$</p>
                  <span class="tab">In matrix notation, if \(z=[Z_1,…,Z_n ]^T\) then \(z^Tz=\sum_{i=1}^n Z_i^2\)  so that \( \mathcal{X}^2=z^T z \sim \chi_n^2\).</span>
                  <br />
                  <br />
                  <p>2. If \(\mathcal{X}^2\) has the distribution \(\chi^2 (n)\), then its expected value is \(E(\mathcal{X}^2)=n\) and its variance is \(Var(\mathcal{X}^2)=2 \cdot n\).</p>
                  <p>3. If \(Y_1,…,Y_n\) are independent Normally distributed random variables each with the distribution \(Y_i \sim \mathcal{N}(\mu_i,\sigma_i^2)\) then:</p>
                  <p>$$ \mathcal{X}^2= \sum_{i=1}^n (\frac{Y_i-\mu_i}{\sigma_i})^2 \sim \chi_n^2 $$</p>
                  <span class="tab">because each of the variables \(Z_i= \frac{Y_i-\mu_i}{\sigma_i}\) has the standard Normal distribution \(\mathcal{N}(0,1)\).</span>
                  <br />
                  <br />
                  <p>4.	Let \(Z_1,…,Z_n\) be independent random variables each with the distribution \(\mathcal{N}(0,1)\) and let \(Y_i=Z_i+\mu_i\), where at least one of the \(\mu_i\)’s is non-zero. Then the distribution of:</p>
                  <p>$$ \sum Y_i^2=\sum (Z_i+\mu_i)^2=\sum Z_i^2+2 \cdot \sum Z_i \cdot \mu_i+ \sum \mu_i^2  $$</p>
                  <span class="tab">has as larger mean \(n + \lambda\) and larger variance \(2 \cdot n + 4 \cdot \lambda\) than \(\chi_n^2\) where \(\lambda = \sum \mu_i^2\). This is called the <strong>non-central chi-squared distribution</strong> with \(n\) degrees of freedom and <strong>non-centrality parameter</strong> \(\lambda\). It is denoted by \(\chi_n^2 (\lambda)\).</span>
                  <br />
                  <br />
                  <p>5. Suppose that the \(Y_i\)’s are not necessarily independent and the vector \(y=[Y_1,…,Y_n ]^T\) has the multivariate normal distribution \(y \sim \mathcal{N}(\mu,V)\) where the variance-covariance matrix \(V\) is non-singular and its inverse is \(V^{-1}\). Then:</p>
                  <p>$$ \mathcal{X}^2=(y-\mu)^Τ V^{-1} (y-\mu) \sim \chi_n^2 $$</p>
                  <p>6. More generally if \(y \sim \mathcal{N}(\mu,V)\) then the random variable \(y^Τ V^{-1} y\) has the non-central chi-squared distribution \(\chi_n^2 (\lambda)\) where \(\lambda=\mu^Τ V^{-1} \mu\).</p>
                  <br />
                  <p>7. If \(X_1^2,…,X_m^2\) are \(m\) independent random variables with the chi-squared distributions \(\mathcal{X}_i^2 \sim \chi_{n_i}^2 (\lambda_i)\), which may or may not be central, then their sum also has a chi-squared distribution with \(\sum n_i\)  degrees of freedom and non-centrality parameter \(\sum \lambda_i\) , i.e.,</p>
                  <p>$$ \sum_{i=1}^m \mathcal{X}_i^2 \sim \chi_{\sum_{i=1}^m n_i}^2 (\sum_{i=1}^m \lambda_i) $$</p>
                  <span class="tab">This is called the <strong>reproductive property</strong> of the chi-squared distribution.</span>
                  <br />
                  <br />
                  <p>8.	Let \(y \sim \mathcal{N}(\mu,V)\), where \(y\) has \(n\) elements but the \(Y_i\)’s are not independent so that \(V\) is singular with rank \(k < n\) and the inverse of \(V\) is not uniquely defined. Let \(V^-\) denote a generalized inverse of \(V\). Then the random variable \(y^T V^- y\) has the non-central chi-squared distribution with \(k\) degrees of freedom and non-centrality parameter \(\lambda=\mu^Τ V^- \mu\).</p>
                </blockquote>
                <br />
                <hr />
                <br />
                <p><strong>a.</strong> First, let us compute the inverse of Variance-Covariance matrix \(V\), \(V^{-1}\), as it will be needed. So:</p>
                <blockquote>
                  <p><u>How to calculate the inverse of a 2x2 matrix:</u></p>
                  <p>A 2x2 matrix has the following general form:</p>
                  <p>\( A=\begin{bmatrix} a & b \\ c & d \end{bmatrix} \)</p>
                  <p>Its inverse then shall be:</p>
                  <p>\( A^{-1}=\frac{1}{det(A)} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix} = \frac{1}{a \cdot d - b \cdot c} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix} \)</p>
                  <p>Here, \(det(A)\) is the determinant of the matrix \(A\), and it must be non-zero for the inverse to exist. It becomes zero in the following cases: </p>
                  <ol class="tab">
                      <li>When the rows (or columns) of the matrix are linearly dependent, meaning one row is a scalar multiple of the other. For example, if the second row is \(k\) times the first row, then the matrix is singular, and the determinant is zero.</li>
                      <li>When the matrix represents a transformation that collapses the space into a lower dimension. For example, if the matrix represents a 2D transformation, a zero determinant means that the transformation squashes the 2D space into a 1D line, resulting in the loss of area (since the transformation is non-invertible). So if the determinant represents the area of the parallelogram formed by the row or column vectors, when the determinant is zero, the parallelogram collapses to a line or point, implying no area.</li>
                  </ol>
                </blockquote>
                <p>\( V^{-1}=\frac{1}{4 \cdot 9-1 \cdot 1} \begin{bmatrix} 9 & -1 \\ -1 & 4 \end{bmatrix} = \frac{1}{35} \begin{bmatrix} 9 & -1 \\ -1 & 4 \end{bmatrix} \Rightarrow V^{-1}=  \begin{bmatrix} \frac{9}{35} & -\frac{1}{35} \\ -\frac{1}{35} & \frac{4}{35} \end{bmatrix} \)</p>
                <p>Since \(y= \begin{bmatrix} Y_1 \\ Y_2 \end{bmatrix}\) and \( \mu=\begin{bmatrix} 2 \\ 3 \end{bmatrix}\), then their difference shall be:</p>
                <p>\( y-\mu=\begin{bmatrix} Y_1 \\ Y_2 \end{bmatrix}-\begin{bmatrix} 2 \\ 3 \end{bmatrix}=\begin{bmatrix} Y_1-2 \\ Y_2-3 \end{bmatrix} \)</p>
                <p>And therefore, the joint distribution, will have the following form:</p>
                <p>$$ (y-\mu)^T V^{-1} (y-\mu)= \begin{bmatrix} Y_1-2 & Y_2-3 \end{bmatrix} \begin{bmatrix} \frac{9}{35} & -\frac{1}{35} \\ -\frac{1}{35} & \frac{4}{35} \end{bmatrix} \begin{bmatrix} Y_1-2 \\ Y_2-3 \end{bmatrix}= $$</p>
                <p>$$ = \begin{bmatrix} \frac{9}{35} (Y_1-2)-\frac{1}{35} (Y_2-3) & -\frac{1}{35} (Y_1-2)+ \frac{4}{35} (Y_2-3) \end{bmatrix} \begin{bmatrix} Y_1-2 \\ Y_2-3 \end{bmatrix}= $$</p>
                <p>$$ = \frac{9}{35} (Y_1-2)^2-\frac{1}{35} (Y_2-3)(Y_1-2)-\frac{1}{35} (Y_1-2)(Y_2-3)+\frac{4}{35} (Y_2-3)^2 \Rightarrow $$</p>
                <p>$$ \Rightarrow (y-μ)^T V^{-1} (y-μ)=\frac{9}{35} (Y_1-2)^2-\frac{2}{35} (Y_1-2)(Y_2-3)+\frac{4}{35} (Y_2-3)^2 $$</p>
                <p>From property 5, we know that for a multivariate normal distribution \(y \sim \mathcal{N}(\mu,V)\), the quadratic form \((y-\mu)^T V^{-1} (y-\mu)\) follows a chi-squared distribution with degrees of freedom equal to the dimension of \(y\) (and in this case, we have only two dimensions), and therefore:</p>
                <p>$$ (y-μ)^T V^{-1} (y-μ)=\frac{9}{35} (Y_1-2)^2-\frac{2}{35} (Y_1-2)(Y_2-3)+\frac{4}{35} (Y_2-3)^2 \sim \chi_2^2 $$</p>
                <br />
                <hr />
                <br />
                <p><strong>b.</strong> From the previous question (a), we already know that:</p>
                <p>\( V^{-1}=  \begin{bmatrix} \frac{9}{35} & -\frac{1}{35} \\ -\frac{1}{35} & \frac{4}{35} \end{bmatrix} \)</p>
                <p>And therefore, the expression for \(y^T V^{-1} y\) shall be:</p>
                <p>$$ y^T V^{-1} y= \begin{bmatrix} Y_1 & Y_2 \end{bmatrix} \begin{bmatrix} \frac{9}{35} & -\frac{1}{35} \\ -\frac{1}{35} & \frac{4}{35} \end{bmatrix} \begin{bmatrix} Y_1 \\ Y_2 \end{bmatrix}= \begin{bmatrix} \frac{9}{35} \cdot Y_1-\frac{1}{35} \cdot Y_2 & -\frac{1}{35} \cdot Y_1 + \frac{4}{35} \cdot Y_2 \end{bmatrix} \begin{bmatrix} Y_1 \\ Y_2 \end{bmatrix}= $$</p>
                <p>$$ = \frac{9}{35} \cdot Y_1^2-\frac{1}{35} \cdot Y_1 \cdot Y_2-\frac{1}{35} \cdot Y_2 \cdot Y_1  + \frac{4}{35} \cdot Y_2^2  \Rightarrow $$</p>
                <p>$$ \Rightarrow y^T V^{-1} y = \frac{9}{35} \cdot Y_1^2-\frac{2}{35} \cdot Y_1 \cdot Y_2  +\frac{4}{35} \cdot Y_2^2 $$</p>
                <p>Now, the distribution of \(y^T V^{-1} y\) is a more general case of the one described in the previous question (a) and thus follows property 6, meaning that: “if \(y \sim \mathcal{N}(μ,V)\) then the random variable \(y^Τ V^{-1} y\) has the non-central chi-squared distribution \(\chi_n^2 (\lambda)\) where \(\lambda=\mu^Τ V^{-1} \mu\).”</p>
                <p>In our case, \(y\) can be written as \(y=\mu+Ζ\), where: \(Ζ \sim \mathcal{N}(0,V)\). So if we expanded on this, we would have:</p>
                <p>\( y^T V^{-1} y=(\mu+Ζ)^T V^{-1} (\mu+Ζ)=\mu^T V^{-1} \mu+2 \cdot \mu^T V^{-1} Z+Z^T V^{-1} Z \)</p>
                <p>with:</p>
                <ul class="tab">
                    <li>	\(Z^T V^{-1} Z \sim \chi_2^2\), because \(Z \sim \mathcal{N}(0,V)\), and the quadratic form of a multivariate normal distribution follows a chi-squared distribution with degrees of freedom equal to the dimension of \(Z\) (which is 2).</li>
                    <li>	\(2 \cdot \mu^T V^{-1} Z\) is normally distributed with a mean of 0.</li>
                </ul>
                <br>
                <br>
                <p>Thus \(y^T V^{-1} y\) is a sum of a chi-squared distribution and a normal distribution. This means that \(y^T V^{-1} y\) follows a non-central chi-squared distribution with 2 degrees of freedom and a non-centrality parameter \(\lambda=\mu^T V^{-1} \mu\), which is:</p>
                <p>$$ \lambda=\mu^T V^{-1} \mu=\begin{bmatrix} 2 & 3 \end{bmatrix} \begin{bmatrix} \frac{9}{35} & -\frac{1}{35} \\ -\frac{1}{35} & \frac{4}{35}) \end{bmatrix} \begin{bmatrix}2 \\ 3 \end{bmatrix}= \begin{bmatrix} \frac{18}{35}-\frac{3}{35} & -\frac{2}{35}+\frac{12}{35}\end{bmatrix} \begin{bmatrix} 2 \\ 3 \end{bmatrix}= $$</p>
                <p>$$ = \begin{bmatrix} \frac{15}{35} & \frac{10}{35} \end{bmatrix} \begin{bmatrix} 2 \\ 3 \end{bmatrix} = \frac{30}{35}+\frac{30}{35}=\frac{60}{35} \Rightarrow \lambda=\frac{12}{7} $$</p>
                <p>Therefore, in conclusion:</p>
                <p>$$ y^T V^{-1} y = \frac{9}{35} \cdot Y_1^2-\frac{2}{35} \cdot Y_1 \cdot Y_2  +\frac{4}{35} \cdot Y_2^2 \sim \chi_2^2 (\frac{12}{7}) $$</p>
                <br />
                <br />
            </section>

            <section id="exercise1.4" class="exercise-section">
                <h3>Exercise 1.4:</h3>
                <p>Let \(Y_1,…,Y_n\) be independent random variables each with the distribution \(\mathcal{N}(\mu,\sigma^2)\). Let:</p>
                <p> $$ \overline{Y} = \frac{1}{n} \sum_{i=1}^n Y_i   \  \   and    \  \     S^2= \frac{1}{n-1} \sum_{i=1}^n (Y_i-\overline{Y})^2. $$</p>
                <p><strong>a.</strong> What is the distribution of \(\overline{Y}\)?</p>
                <p><strong>b.</strong> Show that \(S^2=\frac{1}{n-1} [\sum_{i=1}^n (Y_i-\mu)^2-n(\overline{Y}-\mu)^2 ]\).</p>
                <p><strong>c.</strong> From (b) it follows that \(\sum\frac{(Y_i-\mu)^2}{σ^2} =(n-1)\frac{S^2}{\sigma^2} + [\frac{(\overline{Y}-\mu)^2 n}{\sigma^2} ]\). How does this allow you to deduce that \(Y\) and \(S^2\) are independent?</p>
                <p><strong>d.</strong> What is the distribution of \((n-1)\frac{S^2}{\sigma^2}\)?</p>
                <p><strong>e.</strong> What is the distribution of \(\frac{\overline{Y} - \mu}{\frac{S}{\sqrt{n}}}\) ?</p>
                <h4>Solution:</h4>
                <br />
                <br />
                <p><strong>a.</strong> Since the \(Y_i\) are independent and each has the distribution \(\mathcal{N}(\mu,\sigma^2)\), the expectation of \(\overline{Y}\) is:</p>
                <p>\( Ε(\overline{Y})=Ε[\frac{1}{n} \sum_{i=1}^n Y_i ]=\frac{1}{n} \sum_{i=1}^n E[Y_i]= \frac{1}{n} \cdot n \cdot \mu=\mu \)</p>
                <p>while its variance is:</p>
                <p>\( Var(\overline{Y})=Var(\frac{1}{n} \sum_{i=1}^n Y_i)=\frac{1}{n}^2  \sum_{i=1}^n Var(Y_i) = \frac{1}{n^2} \cdot n \cdot \sigma^2=\frac{\sigma^2}{n} \)</p>
                <p>We know that \(\overline{Y}\) consists of a linear combination of independent, normally distributed variables and therefore it is itself normally distributed. Thus, the distribution of \(\overline{Y}\) shall be:</p>
                <p>$$ \overline{Y} \sim \mathcal{N}(\mu,\frac{\sigma^2}{n}) $$</p>
                <br />
                <hr />
                <br />
                <p><strong>b.</strong> Let us start from the definition of the sample variance:</p>
                <p>$$ S^2 = \frac{1}{n-1} \sum_{i=1}^n (Y_i-\overline{Y})^2 = \frac{1}{n-1} \sum_{i=1}^n [(Y_i-\mu)-(\overline{Y}-\mu)]^2=$$</p>
                <p>$$ = \frac{1}{n-1} \sum_{i=1}^n [(Y_i-\mu)^2 - 2 \cdot (Y_i-\mu)(\overline{Y}-\mu)+(\overline{Y}-\mu)^2 ] = $$</p>
                <p>$$ = \frac{1}{n-1} [\sum_{i=1}^n(Y_i-\mu)^2 - \sum_{i=1}^n 2 \cdot (Y_i-\mu)(\overline{Y}-\mu)+\sum_{i=1}^n(\overline{Y}-\mu)^2]= $$</p>
                <p>$$ = \frac{1}{n-1} [\sum_{i=1}^n(Y_i-\mu)^2 - 2 \cdot (\overline{Y}-\mu) \sum_{i=1}^n(Y_i-\mu)+\sum_{i=1}^n (\overline{Y}-\mu)^2]= $$</p>
                <p>$$ = \frac{1}{n-1} [\sum_{i=1}^n(Y_i-\mu)^2 - 2 \cdot (\overline{Y}-\mu) \cdot n \cdot (\overline{Y}-\mu) + n \cdot (\overline{Y}-\mu)^2]= $$</p>
                <p>$$ = \frac{1}{n-1} [\sum_{i=1}^n(Y_i-\mu)^2 - 2 \cdot n \cdot (\overline{Y}-\mu)^2 + n \cdot (\overline{Y}-\mu)^2 ] \Rightarrow $$</p>
                <p>$$ \Rightarrow S^2 = \frac{1}{n-1} [\sum_{i=1}^n (Y_i-\mu)^2 - n \cdot (\overline{Y}-\mu)^2 ] $$</p>
                <br />
                <hr />
                <br />
                <p><strong>c.</strong> We are given the following expression:</p>
                <p>$$ \frac{\sum_{i=1}^n (Y_i-\mu)^2}{\sigma^2} =\frac{(n-1) S^2}{\sigma^2} + \frac{n(\overline{Y}-\mu)^2}{\sigma^2}  $$</p>
                <p>So, why are Y and S^2 independent? Let us look at the two right hand terms one by one.</p>
                <br />
                <p>Firstly, let us discuss the term:</p>
                <p>\( \frac{(n-1) S^2}{\sigma^2} \)</p>
                <p>Here \(S^2\) is the sample variance, which is defined as:</p>
                <p>\( S^2= \frac{1}{n-1} \sum_{i=1}^n (Y_i-\overline{Y} )^2  \)</p>
                <p>Therefore:</p>
                <p>\( \frac{(n-1) S^2}{\sigma^2} = \frac{n-1}{\sigma^2} \cdot \frac{1}{n-1} \sum_{i=1}^n (Y_i-\overline{Y} )^2 = \frac{1}{\sigma^2}  \sum_{i=1}^n (Y_i-\overline{Y} )^2  \)</p>
                <p>The sample variance measures the spread of the individual \(Y_i\)’s around the sample mean \(\overline{Y}\). This involves \(n-1\) degrees of freedom because the calculation of \(S^2\) depends on \(n\) data points, but the sample mean \(\overline{Y}\) is used to estimate the center of the data, reducing the degrees of freedom by 1.</p>
                <p>Thus, under the assumption that the \(Y_i\)’s are normally distributed, the sum of squares, which was defined above, follows a chi-squared distribution with \(n-1\) degrees of freedom:</p>
                <p>$$ \frac{(n-1) S^2}{\sigma^2} \sim \chi_{n-1}^2 $$</p>
                <br />
                <p>Secondly, let us discuss the term:</p>
                <p>\( \frac{n(\overline{Y}-\mu)^2}{\sigma^2} \)</p>
                <p>And from question <strong>a</strong>, we already know that:</p>
                <p>\( \overline{Y} \sim \mathcal{N}(\mu,\frac{\sigma^2}{n}) \)</p>
                <p>Therefore:</p>
                <p>\( \frac{n(\overline{Y}-\mu)^2}{\sigma^2} = (\frac{\overline{Y}-\mu}{\frac{\sigma}{\sqrt{n}}})^2 \sim Ζ^2 \)</p>
                <p>where \(Z\) is a standard normal random variable, \(Z \sim \mathcal{N}(0,1)\). And hence:</p>
                <p>$$ \frac{n(\overline{Y}-\mu)^2}{\sigma^2} \sim \chi_1^2 $$</p>
                <p>Since the total sum of squares can be split into two independent components, one involving \(\overline{Y}\) and the other involving \(S^2\), then by Cochran’s Theorem, the chi-squared terms must be independent. More formally, the independence of \(\chi_1^2\) and \(\chi_{n-1}^2\) implies that \(\overline{Y}\) and \(S^2\) are independent.</p>
                <br>
                <blockquote>
                  <p><u><i>Cochran’s Theorem:</i></u></p>
                  <p>Cochran's Theorem provides a way to decompose sums of squared normal random variables into independent chi-squared distributions. Specifically, if you have a set of independent normal random variables \(Y_1,Y_2,…,Y_n\) drawn from \(\mathcal{N}(\mu,\sigma^2)\). In our example, Cochran's Theorem states that the total sum of squares:</p>
                  <p>$$ \sum_{i=1}^n (Y_i-\mu)^2  $$</p>
                  <p>can be decomposed into two independent components:</p>
                  <p>\( \frac{n(\overline{Y}-\mu)^2}{\sigma^2} \sim \chi_1^2 \)</p>
                  <p>\( \frac{(n-1) S^2}{\sigma^2} \sim \chi_{n-1}^2\)</p>
                </blockquote>
                <br>
                <p>This result is a key property of normal distributions and is a consequence of the fact that the sample mean \(\overline{Y}\) and sample variance \(S^2\) capture independent aspects of the data. \(\overline{Y}\) captures location (center), while \(S^2\) captures spread (variability) around the center.</p>
                <br />
                <hr />
                <br />
                <p><strong>d.</strong> As it was already shown in question <strong>c</strong>:</p>
                <p>$$ \frac{(n-1) S^2}{\sigma^2} \sim \chi_{n-1}^2 $$</p>
                <br />
                <hr />
                <br />
                <p><strong>e.</strong> From question <strong>c</strong>, we got that:</p>
                <p>\( \frac{n(\overline{Y}-\mu)^2}{\sigma^2} = (\frac{\overline{Y}-\mu}{\frac{\sigma}{\sqrt{n}}})^2 \sim Ζ^2 \Rightarrow \frac{\overline{Y}-\mu}{\frac{\sigma}{\sqrt{n}}} \sim \mathcal{N}(0,1) \)</p>
                <p>\( \frac{(n-1) S^2}{\sigma^2} \sim \chi_{n-1}^2 \Rightarrow S \sim \frac{\sigma}{\sqrt{n-1}} \cdot \sqrt{ \chi_{n-1}^2 } \)</p>
                <p>Thus, the numerator follows a standard normal distribution, while the denominator involves the sample standard deviation, which is related to the chi-squared distribution with \(n-1\) degrees of freedom. So, when we take the ratio of a standard normal random variable and the square root of a chi-squared random variable (divided by its degrees of freedom), the result follows a <strong>t-distribution</strong>. Hence:</p>
                <p>$$ Τ = \frac{\overline{Y}-\mu}{S/\sqrt{n}} \sim \mathcal{t}_{n-1} $$</p>
                <br />
                <br />
            </section>

            <section id="exercise1.5" class="exercise-section">
                <h3>Exercise 1.5:</h3>
                <p>This exercise is a continuation of the example in Section 1.6.2 in which \(Y_1,…,Y_n\) are independent Poisson random variables with the parameter \(\theta\).</p>
                <p><strong>a.</strong> Show that \(E(Y_i)=\theta\) for \(i=1,...,n\).</p>
                <p><strong>b.</strong> Suppose \(\theta=e^\beta\). Find the maximum likelihood estimator of \(\beta\).</p>
                <p><strong>c.</strong> Minimize \(S=\sum (Y_i-e^\beta )^2\)  to obtain a least squares estimator of \(\beta\).</p>
                <h4>Solution:</h4>
                <br />
                <br />
                <p><strong>a.</strong> We are given that \(Y_1,…,Y_n\) are independent Poisson random variables with the parameter \(\theta\). Therefore:</p>
                <p>$$ E(Y_i)=\sum_{k=0}^\infty k \cdot P(Y_i=k) = \sum_{k=0}^\infty k \cdot \frac{\theta^k \cdot e^{-\theta}}{k!} $$</p>
                <p>However, when \(k=0\), the whole term becomes zero, thus it is superfluous in our expression. We can take it out:</p>
                <p>$$ E(Y_i) = \sum_{k=1}^\infty k \cdot \frac{\theta^k \cdot e^{-\theta}}{k!} = \sum_{k=1}^\infty \frac{\theta^k \cdot e^{-\theta}}{(k-1)!} = e^{-\theta} \sum_{k=1}^\infty \frac{\theta^k}{(k-1)!} = e^{-\theta} \sum_{k=1}^\infty \frac{\theta \cdot \theta^{k-1}}{(k-1)!} \Rightarrow $$</p>
                <p>$$ \Rightarrow E(Y_i) = \theta \cdot e^{-\theta} \sum_{k=1}^\infty \frac{\theta^{k-1}}{(k-1)!} $$</p>
                <p>Setting \(j=k-1\), we get:</p>
                <p>$$ E(Y_i) = \theta \cdot e^{-\theta} \sum_{j=0}^\infty \frac{\theta^j}{j!} = \theta \cdot e^{-\theta} \cdot e^\theta \Rightarrow E(Y_i)=\theta $$</p>
                <br />
                <hr />
                <br />
                <p><strong>b.</strong> Given that \(Y_1,…,Y_n\) are independent Poisson random variables with parameter \(\theta=e^\beta\), the probability mass function for each \(Y_i\) is:</p>
                <p>$$ P(Y_i=y_i) = \frac{(e^\beta)^{y_i} \cdot e^{-e^\beta}}{y_i!} $$</p>
                <p>The likelihood function \(L(β)\) is the product of the individual probabilities for all \(Y_i\)'s:</p>
                <p>$$ L(β) = \prod_{i=1}^n P(Y_i=y_i) = \prod_{i=1}^n \frac{(e^\beta)^{y_i} \cdot e^{-e^\beta}}{y_i!} = \frac{(e^\beta)^{\sum_{i=1}^n y_i} \cdot e^{-n \cdot e^\beta}}{\prod_{i=1}^n y_i!} $$</p>
                <p>The log-likelihood function \(l(\beta)\) is the natural logarithm of the likelihood function:</p>
                <p>$$ l(\beta) = lnL(\beta) = ln(\frac{(e^\beta)^{\sum_{i=1}^n y_i} \cdot e^{-n \cdot e^\beta}}{\prod_{i=1}^n y_i!}) = ln[(e^\beta)^{∑_{i=1}^n y_i}] + ln[e^{-n \cdot e^β}] - ln[\prod_{i=1}^n y_i!] \Rightarrow $$</p>
                <p>$$ \Rightarrow l(\beta) = \beta \cdot \sum_{i=1}^n y_i -n \cdot e^β - ln[\prod_{i=1}^n y_i!] $$</p>
                <p>To find the maximum likelihood estimator of \(\beta\), we take the derivative of \(l(\beta)\) with respect to \(\beta\) and set it equal to zero:</p>
                <p>$$ \frac{d}{dβ} l(\beta) = 0 \Rightarrow \frac{d}{dβ} (\beta \cdot \sum_{i=1}^n y_i) - \frac{d}{dβ} (n \cdot e^β) - \frac{d}{dβ} (ln[\prod_{i=1}^n y_i!]) = 0 \Rightarrow $$</p>
                <p>$$ \Rightarrow \sum_{i=1}^n y_i -n \cdot e^\beta-0=0 \Rightarrow e^\beta= \frac{1}{n} \cdot \sum_{i=1}^n y_i \Rightarrow e^\beta = \overline{Y} \Rightarrow ln(e^\beta) = ln(\overline{Y}) \Rightarrow $$</p>
                <p>$$ \Rightarrow \beta = ln(\overline{Y}) $$</p>
                <br />
                <hr />
                <br />
                <p><strong>c.</strong> To minimize \(S\), we need to take the derivative of \(S\) with respect to \(\beta\) and set it equal to zero, so in other words:</p>
                <p>$$ \frac{d}{dβ} (S)=0 \Rightarrow \frac{d}{dβ} [\sum_{i=1}^n (Y_i-e^\beta)^2]=0 \Rightarrow \sum_{i=1}^n 2 \cdot (Y_i-e^\beta) \cdot (-e^\beta)=0 \Rightarrow $$</p>
                <p>$$ \Rightarrow -2 \cdot \sum_{i=1}^n (Y_i-e^\beta) \cdot (e^\beta )=0 \Rightarrow \sum_{i=1}^n (Y_i-e^\beta) \cdot (e^\beta)=0 \Rightarrow \sum_{i=1}^n Y_i \cdot e^\beta - \sum_{i=1}^n e^{2\beta} =0 \Rightarrow $$</p>
                <p>$$ \Rightarrow e^\beta \cdot \sum_{i=1}^n Y_i-n \cdot e^{2\beta}=0 \Rightarrow \sum_{i=1}^n Y_i-n \cdot e^\beta =0 \Rightarrow \sum_{i=1}^n Y_i=n \cdot e^\beta \Rightarrow e^\beta= \frac{1}{n} \cdot \sum_{i=1}^n Y_i \Rightarrow $$</p>
                <p>$$ \Rightarrow \beta = ln(\overline{Y}) $$</p>
                <br />
                <br />
            </section>

            <section id="exercise1.6" class="exercise-section">
                <h3>Exercise 1.6:</h3>
                <p>The data below are the numbers of females and males in the progeny of 16 female light brown apple moths in Muswellbrook, New South Wales, Australia (from Lewis, 1987).</p>
                  <table border="1" cellpadding="5" cellspacing="0">
                    <thead>
                      <tr>
                        <th>Progeny Group</th>
                        <th>Females</th>
                        <th>Males</th>
                      </tr>
                    </thead>
                  <tbody>
                    <tr>
                        <td>1</td>
                        <td>18</td>
                        <td>11</td>
                    </tr>
                    <tr>
                        <td>2</td>
                        <td>31</td>
                        <td>22</td>
                    </tr>
                    <tr>
                        <td>3</td>
                        <td>34</td>
                        <td>27</td>
                    </tr>
                    <tr>
                        <td>4</td>
                        <td>33</td>
                        <td>29</td>
                    </tr>
                    <tr>
                        <td>5</td>
                        <td>27</td>
                        <td>24</td>
                    </tr>
                    <tr>
                        <td>6</td>
                        <td>33</td>
                        <td>29</td>
                    </tr>
                    <tr>
                        <td>7</td>
                        <td>28</td>
                        <td>25</td>
                    </tr>
                    <tr>
                        <td>8</td>
                        <td>23</td>
                        <td>26</td>
                    </tr>
                    <tr>
                        <td>9</td>
                        <td>33</td>
                        <td>38</td>
                    </tr>
                    <tr>
                        <td>10</td>
                        <td>12</td>
                        <td>14</td>
                    </tr>
                    <tr>
                        <td>11</td>
                        <td>19</td>
                        <td>23</td>
                    </tr>
                    <tr>
                        <td>12</td>
                        <td>25</td>
                        <td>31</td>
                    </tr>
                    <tr>
                        <td>13</td>
                        <td>14</td>
                        <td>20</td>
                    </tr>
                    <tr>
                        <td>14</td>
                        <td>4</td>
                        <td>6</td>
                    </tr>
                    <tr>
                        <td>15</td>
                        <td>22</td>
                        <td>34</td>
                    </tr>
                    <tr>
                        <td>16</td>
                        <td>7</td>
                        <td>12</td>
                    </tr>
                </tbody>
                </table>
                <p><strong>a.</strong> Calculate the proportion of females in each of the 16 groups of progeny.</p>
                <p><strong>b.</strong> Let \(Y_i\) denote the number of females and \(n_i\) the number of progeny in each group \((i=1,...,16)\). Suppose the \(Y_i\)’s are independent random variables each with the binomial distribution:</p>
                <p>\( f(y_i;\theta)= \binom{n_i}{y_i} \cdot \theta^{y_i} \cdot (1-\theta)^{n_i-y_i} \)</p>
                <p>Find the maximum likelihood estimator of \(\theta\) using calculus and evaluate it for these data.</p>
                <p><strong>c.</strong> Use a numerical method to estimate \(\hat{\theta}\) and compare the answer with the one from (b).</p>
                <h4>Solution:</h4>
                <br />
                <br />
                <p><strong>a.</strong> </p>
                <p></p>
                <p></p>
                <p></p>
                <br />
                <hr />
                <br />
                <p><strong>b.</strong></p>
                <p></p>
                <p></p>
                <p></p>
                <br />
                <hr />
                <br />
                <p><strong>c.</strong></p>
                <p></p>
                <p></p>
                <p></p>
                <br />
                <br />
            </section>

            <section id="exercise2.1" class="exercise-section">
                <h3>Exercise 2.1:</h3>
                <p></p>
                <h4>Solution:</h4>
                <p></p>
                <p></p>
                <br />
                <br />
            </section>

            <section id="exercise2.2" class="exercise-section">
                <h3>Exercise 2.2:</h3>
                <p></p>
                <h4>Solution:</h4>
                <p></p>
                <p></p>
                <br />
                <br />
            </section>

            <section id="exercise2.3" class="exercise-section">
                <h3>Exercise 2.3:</h3>
                <p></p>
                <h4>Solution:</h4>
                <p></p>
                <p></p>
                <br />
                <br />
            </section>
        </main>
    </div>

    <footer>
        <p>&copy; 2024 Konstantinos Kirillov. All rights reserved.</p>
    </footer>

    <script>
        function showExercise(exerciseId) {
            // Hide all exercise sections
            var exercises = document.getElementsByClassName("exercise-section");
            for (var i = 0; i < exercises.length; i++) {
                exercises[i].style.display = "none";
            }

            // Show the selected exercise
            document.getElementById(exerciseId).style.display = "block";
        }

        // Show the first exercise by default when the page loads
        document.addEventListener("DOMContentLoaded", function() {
            showExercise('exercise1.1');
        });
    </script>
</body>
</html>
